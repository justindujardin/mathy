{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO Problem Solving [![Open Example In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/justindujardin/mathy/blob/master/website/docs/examples/heuristics.ipynb)\n",
    "\n",
    "> This notebook is built using [mathy_envs](https://envs.mathy.ai) and [THE GUY SORRY I WILL FILL THIS IN I PROMISE](https://thelinkomgdoitnow.github.com)'s wonderful Proximal Policy Optimization script.\n",
    "\n",
    "While working with math problems using heuristics is interpretable and reliable, it can be a large engineering task to design combinations of rules and heuristics for handling all the various tree forms that user input questions might take.\n",
    "\n",
    "Rather than invest engineering time into writing heuristics, we can use machine learning algorithms to train a model that can select which actions to take in order to find an optimal path to a solution. Not only is this more robust than random action selections, but it will make solving many types of problems trivial once we get going.\n",
    "\n",
    "Let's look together at how [mathy_envs](https://envs.mathy.ai) can be used with PyTorch to train a problem solving model that can then be used to demonstrate solving problems step-by-step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install mathy_envs>=0.12.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathy Envs Overview\n",
    "\n",
    "Before we get started, let's review how mathy envs works\n",
    "\n",
    "1. Each \"Environment\" is constructed to generate math problem texts, and determine if an expression is \"solved\"\n",
    "2. Users/Models interact with the environments by playing \"episodes\" where they solve problems given a set of rules and environment-specific logic\n",
    "3. Depending on the context the outputs are either used as inputs to a training model, or as an output demonstration for an end-user\n",
    "\n",
    "We'll use the `PolySimplify` environment which generates controllably difficult polynomial simplification problems, and implements logic to determine when they're solved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<mathy_core.parser.ExpressionParser at 0x7f9d040125b0>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List, Optional\n",
    "\n",
    "from mathy_core import (\n",
    "    BaseRule,\n",
    "    ExpressionChangeRule,\n",
    "    ExpressionParser,\n",
    "    MathExpression,\n",
    "    util,\n",
    "    rules as mathy_rules\n",
    ")\n",
    "\n",
    "parser = ExpressionParser()\n",
    "parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PPO Agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requies: torch torchvision torchinfo mathy_envs gymnasium tqdm matplotlib numpy\n",
    "#\n",
    "# Based on: https://github.com/nikhilbarhate99/PPO-PyTorch\n",
    "#\n",
    "# @nikhilbarhate99 ðŸ™‡ - https://github.com/nikhilbarhate99\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "from typing import List, Literal, Optional\n",
    "from pathlib import Path\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Categorical, MultivariateNormal\n",
    "from torchinfo import summary\n",
    "\n",
    "import mathy_envs.gym  # noqa\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class PPOConfig:\n",
    "    env_names: List[str]  # Environment name\n",
    "    state_dim: int  # Dimension of the state space\n",
    "    random_seed: int  # set random seed if required (0 = no random seed)\n",
    "    action_dim: int  # Dimension of the action space\n",
    "    lr_actor: float  # Learning rate for the actor network\n",
    "    lr_critic: float  # Learning rate for the critic network\n",
    "    gamma: float  # Discount factor\n",
    "    K_epochs: int  # Number of epochs to update the policy\n",
    "    eps_clip: float  # Clip parameter for PPO\n",
    "    has_continuous_action_space: bool  # Whether the action space is continuous or discrete\n",
    "    device: torch.device  # Device to run the training on\n",
    "    action_std_init: float = (\n",
    "        0.6  # Initial standard deviation for the action distribution\n",
    "    )\n",
    "    critic_hidden_dim: int = 64  # Dimension of the hidden layer in the critic network\n",
    "\n",
    "\n",
    "class RolloutBuffer:\n",
    "    def __init__(self):\n",
    "        self.actions = []\n",
    "        self.states = []\n",
    "        self.logprobs = []\n",
    "        self.rewards = []\n",
    "        self.state_values = []\n",
    "        self.is_terminals = []\n",
    "\n",
    "    def clear(self):\n",
    "        del self.actions[:]\n",
    "        del self.states[:]\n",
    "        del self.logprobs[:]\n",
    "        del self.rewards[:]\n",
    "        del self.state_values[:]\n",
    "        del self.is_terminals[:]\n",
    "\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        state_dim: int,\n",
    "        hidden_dim: int,\n",
    "        action_dim: int,\n",
    "        has_continuous_action_space: bool,\n",
    "        action_std_init: float,\n",
    "        device: torch.device,\n",
    "    ):\n",
    "        super(ActorCritic, self).__init__()\n",
    "\n",
    "        self.has_continuous_action_space = has_continuous_action_space\n",
    "        self.device = device\n",
    "        self.action_dim = action_dim\n",
    "\n",
    "        if has_continuous_action_space:\n",
    "            self.action_var = torch.full(\n",
    "                (action_dim,), action_std_init * action_std_init\n",
    "            ).to(self.device)\n",
    "\n",
    "        # actor\n",
    "        if has_continuous_action_space:\n",
    "            self.actor = nn.Sequential(\n",
    "                nn.Linear(state_dim, hidden_dim),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(hidden_dim, hidden_dim),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(hidden_dim, action_dim),\n",
    "                nn.Tanh(),\n",
    "            )\n",
    "        else:\n",
    "            self.actor = nn.Sequential(\n",
    "                nn.Linear(state_dim, hidden_dim),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(hidden_dim, hidden_dim),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(hidden_dim, action_dim),\n",
    "                nn.Softmax(dim=-1),\n",
    "            )\n",
    "\n",
    "        # critic\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "        )\n",
    "\n",
    "    def set_action_std(self, new_action_std):\n",
    "        if self.has_continuous_action_space:\n",
    "            self.action_var = torch.full(\n",
    "                (self.action_dim,), new_action_std * new_action_std\n",
    "            ).to(self.device)\n",
    "        else:\n",
    "            print(\n",
    "                \"--------------------------------------------------------------------------------------------\"\n",
    "            )\n",
    "            print(\n",
    "                \"WARNING : Calling ActorCritic::set_action_std() on discrete action space policy\"\n",
    "            )\n",
    "            print(\n",
    "                \"--------------------------------------------------------------------------------------------\"\n",
    "            )\n",
    "\n",
    "    def forward(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def act(self, state):\n",
    "        if self.has_continuous_action_space:\n",
    "            action_mean = self.actor(state)\n",
    "            cov_mat = torch.diag(self.action_var).unsqueeze(dim=0)\n",
    "            dist = MultivariateNormal(action_mean, cov_mat)\n",
    "        else:\n",
    "            action_probs = self.actor(state)\n",
    "            use_mask = True\n",
    "            if use_mask:\n",
    "                mask = state[-action_probs.shape[0] :]\n",
    "                action_probs = action_probs * mask\n",
    "                action_probs = action_probs / torch.sum(action_probs)\n",
    "            dist = Categorical(action_probs)\n",
    "\n",
    "        action = dist.sample()\n",
    "        action_logprob = dist.log_prob(action)\n",
    "        state_val = self.critic(state)\n",
    "\n",
    "        return action.detach(), action_logprob.detach(), state_val.detach()\n",
    "\n",
    "    def evaluate(self, state, action):\n",
    "        if self.has_continuous_action_space:\n",
    "            action_mean = self.actor(state)\n",
    "            action_var = self.action_var.expand_as(action_mean)\n",
    "            cov_mat = torch.diag_embed(action_var).to(self.device)\n",
    "            dist = MultivariateNormal(action_mean, cov_mat)\n",
    "\n",
    "            # for single action continuous environments\n",
    "            if self.action_dim == 1:\n",
    "                action = action.reshape(-1, self.action_dim)\n",
    "\n",
    "        else:\n",
    "            action_probs = self.actor(state)\n",
    "            dist = Categorical(action_probs)\n",
    "\n",
    "        action_logprobs = dist.log_prob(action)\n",
    "        dist_entropy = dist.entropy()\n",
    "        state_values = self.critic(state)\n",
    "\n",
    "        return action_logprobs, state_values, dist_entropy\n",
    "\n",
    "\n",
    "class PPO:\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_dim: int,\n",
    "        action_dim: int,\n",
    "        lr_actor: float,\n",
    "        lr_critic: float,\n",
    "        gamma,\n",
    "        K_epochs: int,\n",
    "        eps_clip,\n",
    "        has_continuous_action_space: bool,\n",
    "        device: torch.device,\n",
    "        action_std_init: float = 0.6,\n",
    "        critic_hidden_dim: int = 64,\n",
    "    ):\n",
    "        self.has_continuous_action_space = has_continuous_action_space\n",
    "        self.device = device\n",
    "\n",
    "        if has_continuous_action_space:\n",
    "            self.action_std = action_std_init\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.eps_clip = eps_clip\n",
    "        self.K_epochs = K_epochs\n",
    "\n",
    "        self.buffer = RolloutBuffer()\n",
    "\n",
    "        self.policy = ActorCritic(\n",
    "            state_dim=state_dim,\n",
    "            action_dim=action_dim,\n",
    "            hidden_dim=critic_hidden_dim,\n",
    "            has_continuous_action_space=has_continuous_action_space,\n",
    "            action_std_init=action_std_init,\n",
    "            device=device,\n",
    "        ).to(device)\n",
    "        self.optimizer = torch.optim.Adam(\n",
    "            [\n",
    "                {\"params\": self.policy.actor.parameters(), \"lr\": lr_actor},\n",
    "                {\"params\": self.policy.critic.parameters(), \"lr\": lr_critic},\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.policy_old = ActorCritic(\n",
    "            state_dim=state_dim,\n",
    "            action_dim=action_dim,\n",
    "            hidden_dim=critic_hidden_dim,\n",
    "            has_continuous_action_space=has_continuous_action_space,\n",
    "            action_std_init=action_std_init,\n",
    "            device=device,\n",
    "        ).to(device)\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "\n",
    "        self.MseLoss = nn.MSELoss()\n",
    "\n",
    "    def set_action_std(self, new_action_std):\n",
    "        if self.has_continuous_action_space:\n",
    "            self.action_std = new_action_std\n",
    "            self.policy.set_action_std(new_action_std)\n",
    "            self.policy_old.set_action_std(new_action_std)\n",
    "\n",
    "        else:\n",
    "            print(\n",
    "                \"--------------------------------------------------------------------------------------------\"\n",
    "            )\n",
    "            print(\n",
    "                \"WARNING : Calling PPO::set_action_std() on discrete action space policy\"\n",
    "            )\n",
    "            print(\n",
    "                \"--------------------------------------------------------------------------------------------\"\n",
    "            )\n",
    "\n",
    "    def decay_action_std(self, action_std_decay_rate, min_action_std):\n",
    "        print(\n",
    "            \"--------------------------------------------------------------------------------------------\"\n",
    "        )\n",
    "\n",
    "        if self.has_continuous_action_space:\n",
    "            self.action_std = self.action_std - action_std_decay_rate\n",
    "            self.action_std = round(self.action_std, 4)\n",
    "            if self.action_std <= min_action_std:\n",
    "                self.action_std = min_action_std\n",
    "                print(\n",
    "                    \"setting actor output action_std to min_action_std : \",\n",
    "                    self.action_std,\n",
    "                )\n",
    "            else:\n",
    "                print(\"setting actor output action_std to : \", self.action_std)\n",
    "            self.set_action_std(self.action_std)\n",
    "\n",
    "        else:\n",
    "            print(\n",
    "                \"WARNING : Calling PPO::decay_action_std() on discrete action space policy\"\n",
    "            )\n",
    "\n",
    "        print(\n",
    "            \"--------------------------------------------------------------------------------------------\"\n",
    "        )\n",
    "\n",
    "    def select_action(self, state):\n",
    "        if self.has_continuous_action_space:\n",
    "            with torch.no_grad():\n",
    "                state = torch.FloatTensor(state).to(self.device)\n",
    "                action, action_logprob, state_val = self.policy_old.act(state)\n",
    "\n",
    "            self.buffer.states.append(state)\n",
    "            self.buffer.actions.append(action)\n",
    "            self.buffer.logprobs.append(action_logprob)\n",
    "            self.buffer.state_values.append(state_val)\n",
    "\n",
    "            return action.detach().cpu().numpy().flatten()\n",
    "\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                state = torch.FloatTensor(state).to(self.device)\n",
    "                action, action_logprob, state_val = self.policy_old.act(state)\n",
    "\n",
    "            self.buffer.states.append(state)\n",
    "            self.buffer.actions.append(action)\n",
    "            self.buffer.logprobs.append(action_logprob)\n",
    "            self.buffer.state_values.append(state_val)\n",
    "\n",
    "            return action.item()\n",
    "\n",
    "    def update(self):\n",
    "        # Monte Carlo estimate of returns\n",
    "        rewards = []\n",
    "        discounted_reward = 0\n",
    "        for reward, is_terminal in zip(\n",
    "            reversed(self.buffer.rewards), reversed(self.buffer.is_terminals)\n",
    "        ):\n",
    "            if is_terminal:\n",
    "                discounted_reward = 0\n",
    "            discounted_reward = reward + (self.gamma * discounted_reward)\n",
    "            rewards.insert(0, discounted_reward)\n",
    "\n",
    "        # Normalizing the rewards\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32).to(self.device)\n",
    "        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-7)\n",
    "\n",
    "        # convert list to tensor\n",
    "        old_states = (\n",
    "            torch.squeeze(torch.stack(self.buffer.states, dim=0))\n",
    "            .detach()\n",
    "            .to(self.device)\n",
    "        )\n",
    "        old_actions = (\n",
    "            torch.squeeze(torch.stack(self.buffer.actions, dim=0))\n",
    "            .detach()\n",
    "            .to(self.device)\n",
    "        )\n",
    "        old_logprobs = (\n",
    "            torch.squeeze(torch.stack(self.buffer.logprobs, dim=0))\n",
    "            .detach()\n",
    "            .to(self.device)\n",
    "        )\n",
    "        old_state_values = (\n",
    "            torch.squeeze(torch.stack(self.buffer.state_values, dim=0))\n",
    "            .detach()\n",
    "            .to(self.device)\n",
    "        )\n",
    "\n",
    "        # calculate advantages\n",
    "        advantages = rewards.detach() - old_state_values.detach()\n",
    "\n",
    "        # Optimize policy for K epochs\n",
    "        for _ in range(self.K_epochs):\n",
    "            # Evaluating old actions and values\n",
    "            logprobs, state_values, dist_entropy = self.policy.evaluate(\n",
    "                old_states, old_actions\n",
    "            )\n",
    "\n",
    "            # match state_values tensor dimensions with rewards tensor\n",
    "            state_values = torch.squeeze(state_values)\n",
    "\n",
    "            # Finding the ratio (pi_theta / pi_theta__old)\n",
    "            ratios = torch.exp(logprobs - old_logprobs.detach())\n",
    "\n",
    "            # Finding Surrogate Loss\n",
    "            surr1 = ratios * advantages\n",
    "            surr2 = (\n",
    "                torch.clamp(ratios, 1 - self.eps_clip, 1 + self.eps_clip) * advantages\n",
    "            )\n",
    "\n",
    "            # final loss of clipped objective PPO\n",
    "            loss = (\n",
    "                -torch.min(surr1, surr2)\n",
    "                + 0.5 * self.MseLoss(state_values, rewards)\n",
    "                - 0.01 * dist_entropy\n",
    "            )\n",
    "\n",
    "            # take gradient step\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.mean().backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "        # Copy new weights into old policy\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "\n",
    "        # clear buffer\n",
    "        self.buffer.clear()\n",
    "\n",
    "    def save(self, checkpoint_path):\n",
    "        torch.save(self.policy_old.state_dict(), checkpoint_path)\n",
    "\n",
    "    def load(self, checkpoint_path):\n",
    "        self.policy_old.load_state_dict(\n",
    "            torch.load(checkpoint_path, map_location=lambda storage, loc: storage)\n",
    "        )\n",
    "        self.policy.load_state_dict(\n",
    "            torch.load(checkpoint_path, map_location=lambda storage, loc: storage)\n",
    "        )\n",
    "\n",
    "\n",
    "################################### Training ###################################\n",
    "def train(config: PPOConfig, init_from: Optional[str] = None) -> None:\n",
    "    print(f\"Device set to: {torch.cuda.get_device_name(config.device)}\")\n",
    "    print(\n",
    "        \"============================================================================================\"\n",
    "    )\n",
    "    print(f\"training environments : {', '.join(config.env_names)}\")\n",
    "\n",
    "    ####### initialize environment hyperparameters ######\n",
    "\n",
    "    max_ep_len = 1000  # max timesteps in one episode\n",
    "    max_training_timesteps = int(\n",
    "        50e6\n",
    "    )  # break training loop if timeteps > max_training_timesteps\n",
    "\n",
    "    print_freq = max_ep_len * 10  # print avg reward in the interval (in num timesteps)\n",
    "    log_freq = max_ep_len * 2  # log avg reward in the interval (in num timesteps)\n",
    "    save_model_freq = int(1e5)  # save model frequency (in num timesteps)\n",
    "\n",
    "    action_std = 0.6  # starting std for action distribution (Multivariate Normal)\n",
    "    action_std_decay_rate = 0.05  # linearly decay action_std (action_std = action_std - action_std_decay_rate)\n",
    "    min_action_std = (\n",
    "        0.1  # minimum action_std (stop decay after action_std <= min_action_std)\n",
    "    )\n",
    "    action_std_decay_freq = int(2.5e5)  # action_std decay frequency (in num timesteps)\n",
    "    ################ PPO hyperparameters ################\n",
    "    update_timestep = max_ep_len * 4  # update policy every n timesteps\n",
    "    #####################################################\n",
    "\n",
    "    envs = [\n",
    "        gym.make(name, invalid_action_response=\"raise\", verbose=False)\n",
    "        for name in config.env_names\n",
    "    ]\n",
    "    assert len(envs) > 0, \"No environments found\"\n",
    "    env = envs[0]\n",
    "\n",
    "    # state space dimension\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "\n",
    "    # action space dimension\n",
    "    if config.has_continuous_action_space:\n",
    "        action_dim = env.action_space.shape[0]\n",
    "    else:\n",
    "        action_dim = env.action_space.n\n",
    "\n",
    "    env_short = config.env_names[0] if len(config.env_names) == 1 else \"multi\"\n",
    "    root_path = Path(\"./trained\") / env_short\n",
    "    ###################### logging ######################\n",
    "    #### log files for multiple runs are NOT overwritten\n",
    "    log_dir = root_path / \"logs\"\n",
    "    log_dir.mkdir(parents=True, exist_ok=True)\n",
    "    #### get number of log files in log directory\n",
    "    run_num = 0\n",
    "    current_num_files = next(os.walk(log_dir))[2]\n",
    "    run_num = len(current_num_files)\n",
    "\n",
    "    #### create new log file for each run\n",
    "    log_f_name = f\"{log_dir}/log_{run_num}.csv\"\n",
    "    print(f\"current logging run number for {env_short}: {run_num}\")\n",
    "    print(f\"logging at: {log_f_name}\")\n",
    "    #####################################################\n",
    "\n",
    "    ################### checkpointing ###################\n",
    "    checkpoint_path = root_path / \"model.pth\"\n",
    "    print(f\"save checkpoint path: {checkpoint_path}\")\n",
    "    #####################################################\n",
    "\n",
    "    ############# print all hyperparameters #############\n",
    "    print(\n",
    "        \"--------------------------------------------------------------------------------------------\"\n",
    "    )\n",
    "    print(f\"max training timesteps: {max_training_timesteps}\")\n",
    "    print(f\"max timesteps per episode: {max_ep_len}\")\n",
    "    print(f\"model saving frequency: {save_model_freq} timesteps\")\n",
    "    print(f\"log frequency: {log_freq} timesteps\")\n",
    "    print(f\"printing average reward over episodes in last: {print_freq} timesteps\")\n",
    "    print(\n",
    "        \"--------------------------------------------------------------------------------------------\"\n",
    "    )\n",
    "    print(f\"state space dimension: {state_dim}\")\n",
    "    print(f\"action space dimension: {action_dim}\")\n",
    "    print(\n",
    "        \"--------------------------------------------------------------------------------------------\"\n",
    "    )\n",
    "    if config.has_continuous_action_space:\n",
    "        print(\"Initializing a continuous action space policy\")\n",
    "        print(\n",
    "            \"--------------------------------------------------------------------------------------------\"\n",
    "        )\n",
    "        print(f\"starting std of action distribution: {action_std}\")\n",
    "        print(f\"decay rate of std of action distribution: {action_std_decay_rate}\")\n",
    "        print(f\"minimum std of action distribution: {min_action_std}\")\n",
    "        print(\n",
    "            f\"decay frequency of std of action distribution: {action_std_decay_freq} timesteps\"\n",
    "        )\n",
    "    else:\n",
    "        print(\"Initializing a discrete action space policy\")\n",
    "    print(\n",
    "        \"--------------------------------------------------------------------------------------------\"\n",
    "    )\n",
    "    print(f\"PPO update frequency: {update_timestep} timesteps\")\n",
    "    print(f\"PPO K epochs: {config.K_epochs}\")\n",
    "    print(f\"PPO epsilon clip: {config.eps_clip}\")\n",
    "    print(f\"discount factor (gamma): {config.gamma}\")\n",
    "    print(\n",
    "        \"--------------------------------------------------------------------------------------------\"\n",
    "    )\n",
    "    print(f\"optimizer learning rate actor: {config.lr_actor}\")\n",
    "    print(f\"optimizer learning rate critic: {config.lr_critic}\")\n",
    "    if config.random_seed:\n",
    "        print(\n",
    "            \"--------------------------------------------------------------------------------------------\"\n",
    "        )\n",
    "        print(f\"setting random seed to {config.random_seed}\")\n",
    "        torch.manual_seed(config.random_seed)\n",
    "        np.random.seed(config.random_seed)\n",
    "    #####################################################\n",
    "\n",
    "    print(\n",
    "        \"============================================================================================\"\n",
    "    )\n",
    "\n",
    "    print(f\"max training timesteps : {max_training_timesteps}\")\n",
    "    print(f\"max timesteps per episode : {max_ep_len}\")\n",
    "    print(f\"model saving frequency : {save_model_freq} timesteps\")\n",
    "    print(f\"log frequency : {log_freq} timesteps\")\n",
    "    print(f\"printing average reward over episodes in last : {print_freq} timesteps\")\n",
    "    print(\n",
    "        \"--------------------------------------------------------------------------------------------\"\n",
    "    )\n",
    "    print(f\"state space dimension : {state_dim}\")\n",
    "    print(f\"action space dimension : {action_dim}\")\n",
    "    print(\n",
    "        \"--------------------------------------------------------------------------------------------\"\n",
    "    )\n",
    "    if config.has_continuous_action_space:\n",
    "        print(\"Initializing a continuous action space policy\")\n",
    "        print(\n",
    "            \"--------------------------------------------------------------------------------------------\"\n",
    "        )\n",
    "        print(f\"starting std of action distribution : {action_std}\")\n",
    "        print(f\"decay rate of std of action distribution : {action_std_decay_rate}\")\n",
    "        print(f\"minimum std of action distribution : {min_action_std}\")\n",
    "        print(\n",
    "            f\"decay frequency of std of action distribution : {action_std_decay_freq} timesteps\"\n",
    "        )\n",
    "    else:\n",
    "        print(\"Initializing a discrete action space policy\")\n",
    "    print(\n",
    "        \"--------------------------------------------------------------------------------------------\"\n",
    "    )\n",
    "    print(f\"PPO update frequency : {update_timestep} timesteps\")\n",
    "    print(f\"PPO K epochs : {config.K_epochs}\")\n",
    "    print(f\"PPO epsilon clip : {config.eps_clip}\")\n",
    "    print(f\"discount factor (gamma) : {config.gamma}\")\n",
    "    print(\n",
    "        \"--------------------------------------------------------------------------------------------\"\n",
    "    )\n",
    "    print(f\"optimizer learning rate actor : {config.lr_actor}\")\n",
    "    print(f\"optimizer learning rate critic : {config.lr_critic}\")\n",
    "    if config.random_seed:\n",
    "        print(\n",
    "            \"--------------------------------------------------------------------------------------------\"\n",
    "        )\n",
    "        print(f\"setting random seed to :{config.random_seed}\")\n",
    "        torch.manual_seed(config.random_seed)\n",
    "        np.random.seed(config.random_seed)\n",
    "    #####################################################\n",
    "\n",
    "    print(\n",
    "        \"============================================================================================\"\n",
    "    )\n",
    "\n",
    "    ################# training procedure ################\n",
    "\n",
    "    # initialize a PPO agent\n",
    "    ppo_agent = PPO(\n",
    "        state_dim,\n",
    "        action_dim,\n",
    "        config.lr_actor,\n",
    "        config.lr_critic,\n",
    "        config.gamma,\n",
    "        config.K_epochs,\n",
    "        config.eps_clip,\n",
    "        config.has_continuous_action_space,\n",
    "        config.device,\n",
    "        action_std,\n",
    "        critic_hidden_dim=config.critic_hidden_dim,\n",
    "    )\n",
    "    summary(ppo_agent.policy.actor, input_size=(state_dim,))\n",
    "    summary(ppo_agent.policy.critic, input_size=(state_dim,))\n",
    "\n",
    "    if init_from is not None:\n",
    "        print(f\"loading network from : {init_from}\")\n",
    "        ppo_agent.load(init_from)\n",
    "        print(\n",
    "            \"--------------------------------------------------------------------------------------------\"\n",
    "        )\n",
    "\n",
    "    # track total training time\n",
    "    start_time = datetime.now().replace(microsecond=0)\n",
    "    print(f\"Started training at (GMT) : {start_time}\")\n",
    "\n",
    "    print(\n",
    "        \"============================================================================================\",\n",
    "        flush=True,\n",
    "    )\n",
    "\n",
    "    # logging file\n",
    "    log_f = open(log_f_name, \"w+\")\n",
    "    log_f.write(\"episode,timestep,reward\\n\")\n",
    "\n",
    "    # printing and logging variables\n",
    "    print_running_reward = 0\n",
    "    print_running_episodes = 0\n",
    "\n",
    "    log_running_reward = 0\n",
    "    log_running_episodes = 0\n",
    "\n",
    "    time_step = 0\n",
    "    i_episode = 0\n",
    "\n",
    "    # training loop\n",
    "    while time_step <= max_training_timesteps:\n",
    "        # Choose a random env\n",
    "        env = np.random.choice(envs)\n",
    "\n",
    "        state, _ = env.reset()\n",
    "        current_ep_reward = 0\n",
    "\n",
    "        for t in range(1, max_ep_len + 1):\n",
    "            # select action with policy\n",
    "            action = ppo_agent.select_action(state)\n",
    "            state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            # saving reward and is_terminals\n",
    "            ppo_agent.buffer.rewards.append(reward)\n",
    "            ppo_agent.buffer.is_terminals.append(done)\n",
    "\n",
    "            time_step += 1\n",
    "            current_ep_reward += float(reward)\n",
    "\n",
    "            # update PPO agent\n",
    "            if time_step % update_timestep == 0:\n",
    "                ppo_agent.update()\n",
    "\n",
    "            # if continuous action space; then decay action std of ouput action distribution\n",
    "            if (\n",
    "                config.has_continuous_action_space\n",
    "                and time_step % action_std_decay_freq == 0\n",
    "            ):\n",
    "                ppo_agent.decay_action_std(action_std_decay_rate, min_action_std)\n",
    "\n",
    "            # log in logging file\n",
    "            if time_step % log_freq == 0:\n",
    "                # log average reward till last episode\n",
    "                log_avg_reward = log_running_reward / log_running_episodes\n",
    "                log_avg_reward = round(log_avg_reward, 4)\n",
    "\n",
    "                log_f.write(\"{},{},{}\\n\".format(i_episode, time_step, log_avg_reward))\n",
    "                log_f.flush()\n",
    "\n",
    "                log_running_reward = 0\n",
    "                log_running_episodes = 0\n",
    "\n",
    "            # printing average reward\n",
    "            if time_step % print_freq == 0:\n",
    "                # print average reward till last episode\n",
    "                print_avg_reward = print_running_reward / print_running_episodes\n",
    "                print_avg_reward = round(print_avg_reward, 2)\n",
    "\n",
    "                print(\n",
    "                    f\"Episode : {i_episode} \\t\\t Timestep : {time_step} \\t\\t Average Reward : {print_avg_reward}\",\n",
    "                    flush=True,\n",
    "                )\n",
    "\n",
    "                print_running_reward = 0\n",
    "                print_running_episodes = 0\n",
    "\n",
    "            # save model weights\n",
    "            if time_step % save_model_freq == 0:\n",
    "                print(\n",
    "                    \"--------------------------------------------------------------------------------------------\"\n",
    "                )\n",
    "                print(f\"saving model at : {checkpoint_path}\")\n",
    "                ppo_agent.save(checkpoint_path)\n",
    "                print(\"model saved\")\n",
    "                print(\n",
    "                    f\"Elapsed Time  : {datetime.now().replace(microsecond=0) - start_time}\"\n",
    "                )\n",
    "                print(\n",
    "                    \"--------------------------------------------------------------------------------------------\",\n",
    "                    flush=True,\n",
    "                )\n",
    "\n",
    "            # break; if the episode is over\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        print_running_reward += current_ep_reward\n",
    "        print_running_episodes += 1\n",
    "\n",
    "        log_running_reward += current_ep_reward\n",
    "        log_running_episodes += 1\n",
    "\n",
    "        i_episode += 1\n",
    "\n",
    "    log_f.close()\n",
    "    env.close()\n",
    "\n",
    "    # print total training time\n",
    "    print(\n",
    "        \"============================================================================================\"\n",
    "    )\n",
    "    end_time = datetime.now().replace(microsecond=0)\n",
    "    print(\"Started training at (GMT) : \", start_time)\n",
    "    print(\"Finished training at (GMT) : \", end_time)\n",
    "    print(\"Total training time  : \", end_time - start_time)\n",
    "    print(\n",
    "        \"============================================================================================\"\n",
    "    )\n",
    "\n",
    "\n",
    "def test(config: PPOConfig, checkpoint_path: str):\n",
    "    print(\n",
    "        \"============================================================================================\"\n",
    "    )\n",
    "\n",
    "    envs = [\n",
    "        gym.make(name, invalid_action_response=\"raise\", verbose=True)\n",
    "        for name in config.env_names\n",
    "    ]\n",
    "    assert len(envs) > 0, \"No environments found\"\n",
    "    env = envs[0]\n",
    "\n",
    "    # state space dimension\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "\n",
    "    # action space dimension\n",
    "    if config.has_continuous_action_space:\n",
    "        action_dim = env.action_space.shape[0]\n",
    "    else:\n",
    "        action_dim = env.action_space.n\n",
    "\n",
    "    # initialize a PPO agent\n",
    "    ppo_agent = PPO(\n",
    "        state_dim,\n",
    "        action_dim,\n",
    "        config.lr_actor,\n",
    "        config.lr_critic,\n",
    "        config.gamma,\n",
    "        config.K_epochs,\n",
    "        config.eps_clip,\n",
    "        config.has_continuous_action_space,\n",
    "        config.device,\n",
    "    )\n",
    "\n",
    "    print(\"loading network from : \" + checkpoint_path)\n",
    "    ppo_agent.load(checkpoint_path)\n",
    "    print(\n",
    "        \"--------------------------------------------------------------------------------------------\"\n",
    "    )\n",
    "\n",
    "    test_running_reward = 0\n",
    "    total_test_episodes = 1000  # total num of testing episodes\n",
    "    for ep in range(1, total_test_episodes + 1):\n",
    "        env = np.random.choice(envs)\n",
    "        ep_reward = 0\n",
    "        print(env.mathy.get_env_namespace())\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = ppo_agent.select_action(state)\n",
    "            state, reward, terminated, truncated, _ = env.step(action)\n",
    "            # time.sleep(0.15)\n",
    "            done = terminated or truncated\n",
    "            ep_reward += float(reward)\n",
    "\n",
    "        # clear buffer\n",
    "        ppo_agent.buffer.clear()\n",
    "        test_running_reward += ep_reward\n",
    "\n",
    "        emoji = \"âœ…\" if ep_reward >= 1.3 else \"ðŸŸ¨\" if ep_reward >= 0.6 else \"ðŸ”´\"\n",
    "        spacer = \"=\" * 100\n",
    "        print(f\"{ep} {spacer} {emoji} Reward: {round(ep_reward, 2)}\")\n",
    "        ep_reward = 0\n",
    "\n",
    "    env.close()\n",
    "\n",
    "    print(\n",
    "        \"============================================================================================\"\n",
    "    )\n",
    "\n",
    "    avg_test_reward = test_running_reward / total_test_episodes\n",
    "    avg_test_reward = round(avg_test_reward, 2)\n",
    "    print(\"average test reward : \" + str(avg_test_reward))\n",
    "\n",
    "    print(\n",
    "        \"============================================================================================\"\n",
    "    )\n",
    "\n",
    "\n",
    "EnvTypes = Literal[\n",
    "    \"poly\",\n",
    "    \"poly-blockers\",\n",
    "    \"poly-combine\",\n",
    "    \"poly-commute\",\n",
    "    \"poly-grouping\",\n",
    "    \"poly-like-terms-haystack\",\n",
    "    \"binomial\",\n",
    "    \"complex\",\n",
    "]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env_difficulty: Literal[\"easy\", \"normal\", \"hard\"] = \"easy\"\n",
    "    env_names = [\n",
    "        f\"mathy-{t}-{env_difficulty}-v0\"\n",
    "        for t in [\n",
    "            \"poly\",\n",
    "            \"poly-blockers\",\n",
    "            \"poly-combine\",\n",
    "            \"poly-commute\",\n",
    "            \"poly-grouping\",\n",
    "            \"poly-like-terms-haystack\",\n",
    "            \"binomial\",\n",
    "            \"complex\",\n",
    "        ]\n",
    "    ]\n",
    "    config = PPOConfig(\n",
    "        env_names=env_names,\n",
    "        has_continuous_action_space=False,  # continuous action space; else discrete\n",
    "        random_seed=1337,  # set random seed if required (0 = no random seed)\n",
    "        state_dim=0,\n",
    "        action_dim=0,\n",
    "        lr_actor=0.0003,  # learning rate for actor network\n",
    "        lr_critic=0.001,  # learning rate for critic network\n",
    "        gamma=0.99,  # discount factor\n",
    "        K_epochs=80,  # update policy for K epochs in one PPO update\n",
    "        eps_clip=0.2,  # clip parameter for PPO\n",
    "        device=torch.device(\"cpu\" if not torch.cuda.is_available() else \"cuda:0\"),\n",
    "    )\n",
    "    if len(sys.argv) == 2:\n",
    "        test(config, sys.argv[1])\n",
    "    else:\n",
    "        train(config, sys.argv[1] if len(sys.argv) > 1 else None)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
