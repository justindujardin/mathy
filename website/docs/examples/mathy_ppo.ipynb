{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Solver [![Open Example In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/justindujardin/mathy/blob/master/website/docs/examples/mathy_ppo.ipynb)\n",
    "\n",
    "> This notebook is built using [mathy_envs](https://envs.mathy.ai) and a modified version of [@nikhilbarhate99](https://github.com/nikhilbarhate99)'s wonderful [PPO-Pytorch](https://github.com/nikhilbarhate99/PPO-PyTorch) script.\n",
    "\n",
    "While working with math problems using heuristics is interpretable and reliable, it can be a large engineering task to design combinations of rules and heuristics for handling all the various tree forms that user input questions might take.\n",
    "\n",
    "Rather than invest engineering time into writing heuristics, we can use machine learning algorithms to train a model that can select which actions to take in order to find an optimal path to a solution. Not only is this more robust than random action selections, but it will make solving many types of problems trivial once we get going.\n",
    "\n",
    "Let's look together at how [mathy_envs](https://envs.mathy.ai) can be used with Proximal Policy Optimization (PPO) in PyTorch to train a problem solving model that can then be used to demonstrate solving problems step-by-step. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install mathy_envs>=0.12.1 torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Before we get started, let's review what mathy envs are and how they work. Mathy envs are reinforcement learning environments for maniuplating math trees with a rules system.\n",
    "\n",
    "1. Each mathy_envs environment generates math problem texts and determines if the current expression is \"solved\" or not\n",
    "2. Users/Models interact with the environments by playing \"episodes\" where they solve problems given a set of rules and environment-specific logic\n",
    "3. Depending on the context the outputs are either used as inputs to a training model, or as an output demonstration for an end-user\n",
    "\n",
    "We're going to use reinforcement learning to train a model that is capable of solving problems generated by the mathy_envs library.\n",
    "\n",
    "Specifically we choose the `PolySimplify` environment which generates controllably difficult polynomial simplification problems, and implements logic to determine when they're solved. \n",
    "\n",
    "For the machine learning portion, we choose the Proximal Policy Optimization algorithm, which is an online learning algorithm.\n",
    "\n",
    "Before we get into the machine learning parts, let's quickly get a taste for the basics of our environments. \n",
    "\n",
    "Mathy envs implements a base environment interface, and that's wrapped in a set of classes exposed for gym/gymnasium libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment: mathy.polynomials.simplify\n",
      "Num Actions: 896\n",
      "Rules      : ['Constant Arithmetic', 'Commutative Swap', 'Distributive Multiply', 'Distributive Factoring', 'Associative Group', 'Variable Multiplication', 'Restate Subtraction']\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "from mathy_envs import MathyEnv\n",
    "from mathy_envs.gym import MathyGymEnv\n",
    "\n",
    "# Environment difficulty level (options: 'easy', 'normal', 'hard')\n",
    "env_difficulty = \"easy\"\n",
    "\n",
    "# Environment names to train on, based on environment types and difficulty\n",
    "env_types = [\n",
    "    \"poly\",\n",
    "    # \"poly-blockers\",\n",
    "    # \"poly-combine\",\n",
    "    # \"poly-commute\",\n",
    "    # \"poly-grouping\",\n",
    "    # \"poly-like-terms-haystack\",\n",
    "    # \"binomial\",\n",
    "    # \"complex\",\n",
    "]\n",
    "env_names = [f\"mathy-{t}-{env_difficulty}-v0\" for t in env_types]\n",
    "\n",
    "env: MathyGymEnv = gym.make(env_names[0])\n",
    "base_env: MathyEnv = env.unwrapped.mathy\n",
    "print(f\"Environment: {base_env.get_env_namespace()}\")\n",
    "print(f\"Num Actions: {base_env.action_size}\")\n",
    "print(f\"Rules      : {[e.name for e in base_env.rules]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proximal Policy Optimization\n",
    "\n",
    "\n",
    "Proximal Policy Optimization (PPO) is a reinforcement learning approach that strikes a balance between the simplicity of implementation and sample efficiency. Developed by John Schulman and his colleagues at OpenAI, PPO is designed to be more stable and reliable than earlier policy gradient methods, thanks to its novel objective function that moderates the policy updates.\n",
    "\n",
    "PPO uses a buffer of trajectories called a rollout buffer to store key elements like states, actions, and rewards generated by interacting with the environments. These buffered trajectories are used when making updates to the actor critic neural networks during training.\n",
    "\n",
    "We'll use PPO here to train an agent that is able to solve polynomial simplification problems in a step-by-step manner.\n",
    "\n",
    "First, let's set some variables that can be changed later for experimentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate for the actor network\n",
    "lr_actor = 0.0003\n",
    "\n",
    "# Learning rate for the critic network\n",
    "lr_critic = 0.001\n",
    "\n",
    "# Discount factor for future rewards\n",
    "gamma = 0.99\n",
    "\n",
    "# Number of epochs to update the policy\n",
    "K_epochs = 80\n",
    "\n",
    "# Clip parameter for PPO, used in policy update\n",
    "eps_clip = 0.2\n",
    "\n",
    "# Random seed setting (0 = no random seed)\n",
    "random_seed = 1337\n",
    "\n",
    "# Device to run the training on (CPU or CUDA)\n",
    "device = torch.device(\"cpu\" if not torch.cuda.is_available() else \"cuda:0\")\n",
    "\n",
    "# Dimension of the hidden layer in the critic network\n",
    "critic_hidden_dim = 64\n",
    "\n",
    "# Where to save the model\n",
    "checkpoint_path = \"ppo.pth\"\n",
    "\n",
    "# Whether or not to use masked action selection. This makes the problems significantly easier when\n",
    "# true because the action space is sparse with most possible actions being invalid. When false, the\n",
    "# agent must learn to avoid invalid actions itself, making the problems much more challenging given\n",
    "# the action space on the order of hundreds or thousands of possible actions for each state.\n",
    "use_masked_actions = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rollout Buffer\n",
    "\n",
    "The Rollout Buffer in PPO stores the trajectory of the agent during its interaction with the environment over a single policy iteration. This includes actions, states, rewards, log probabilities of the actions under the current policy, and state values. When the policy is updated, the buffer is cleared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RolloutBuffer:\n",
    "    def __init__(self):\n",
    "        self.actions = []\n",
    "        self.states = []\n",
    "        self.logprobs = []\n",
    "        self.rewards = []\n",
    "        self.state_values = []\n",
    "        self.is_terminals = []\n",
    "\n",
    "    def clear(self):\n",
    "        del self.actions[:]\n",
    "        del self.states[:]\n",
    "        del self.logprobs[:]\n",
    "        del self.rewards[:]\n",
    "        del self.state_values[:]\n",
    "        del self.is_terminals[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actor-Critic Module\n",
    "\n",
    "The Actor-Critic module in PPO is the core of the policy learning mechanism, and has two key components:\n",
    "\n",
    "- the Actor, which is responsible for choosing actions based on the current state. This replaces the action selection with heuristics we use in previous examples.\n",
    "- the Critic, which evaluates actor actions by estimating the value of the state. This is used to help steer the actor in the direction of higher value actions.\n",
    "\n",
    "The dual structure allows for more efficient and stable learning by combining the strengths of both policy-based and value-based approaches in reinforcement learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.device = device\n",
    "        self.action_dim = action_dim\n",
    "\n",
    "        # Actor network\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(state_dim, critic_hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(critic_hidden_dim, critic_hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(critic_hidden_dim, action_dim),\n",
    "            nn.Softmax(dim=-1),\n",
    "        )\n",
    "\n",
    "        # Critic network\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(state_dim, critic_hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(critic_hidden_dim, critic_hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(critic_hidden_dim, 1),\n",
    "        )\n",
    "\n",
    "    def act(self, state):\n",
    "        action_probs = self.actor(state)\n",
    "        if use_masked_actions:\n",
    "            mask = state[-action_probs.shape[0] :]\n",
    "            action_probs = action_probs * mask\n",
    "            action_probs = action_probs / torch.sum(action_probs)\n",
    "        dist = Categorical(action_probs)\n",
    "        action = dist.sample()\n",
    "        action_logprob = dist.log_prob(action)\n",
    "        state_val = self.critic(state)\n",
    "\n",
    "        return action.detach(), action_logprob.detach(), state_val.detach()\n",
    "\n",
    "    def evaluate(self, state, action):\n",
    "        action_probs = self.actor(state)\n",
    "        dist = Categorical(action_probs)\n",
    "        action_logprobs = dist.log_prob(action)\n",
    "        dist_entropy = dist.entropy()\n",
    "        state_values = self.critic(state)\n",
    "\n",
    "        return action_logprobs, state_values, dist_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm\n",
    "\n",
    "The PPO class encapsulates the Proximal Policy Optimization algorithm, a policy gradient method for reinforcement learning. It manages the training loop, including action selection, policy updating, and handling the experience buffer.\n",
    "\n",
    "The class initializes two ActorCritic models: one for the current policy and another as a reference to the old policy. This structure is crucial for implementing PPO's clipped surrogate objective function, which moderates the policy updates for stability.\n",
    "\n",
    "The class also includes methods for saving and loading model checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class PPO:\n",
    "    def __init__(self, state_dim: int, action_dim: int):\n",
    "        self.device = device\n",
    "        self.gamma = gamma\n",
    "        self.eps_clip = eps_clip\n",
    "        self.K_epochs = K_epochs\n",
    "\n",
    "        self.buffer = RolloutBuffer()\n",
    "        self.policy = ActorCritic(state_dim, action_dim).to(device)\n",
    "        self.optimizer = torch.optim.Adam(\n",
    "            [\n",
    "                {\"params\": self.policy.actor.parameters(), \"lr\": lr_actor},\n",
    "                {\"params\": self.policy.critic.parameters(), \"lr\": lr_critic},\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.policy_old = ActorCritic(state_dim, action_dim).to(device)\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "\n",
    "        self.MseLoss = nn.MSELoss()\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state = torch.FloatTensor(state).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            action, action_logprob, state_val = self.policy_old.act(state)\n",
    "\n",
    "        self.buffer.states.append(state)\n",
    "        self.buffer.actions.append(action)\n",
    "        self.buffer.logprobs.append(action_logprob)\n",
    "        self.buffer.state_values.append(state_val)\n",
    "\n",
    "        return action.item()\n",
    "\n",
    "    def update(self):\n",
    "        # Monte Carlo estimate of returns\n",
    "        rewards = []\n",
    "        discounted_reward = 0\n",
    "        for reward, is_terminal in zip(\n",
    "            reversed(self.buffer.rewards), reversed(self.buffer.is_terminals)\n",
    "        ):\n",
    "            if is_terminal:\n",
    "                discounted_reward = 0\n",
    "            discounted_reward = reward + (self.gamma * discounted_reward)\n",
    "            rewards.insert(0, discounted_reward)\n",
    "\n",
    "        # Normalizing the rewards\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32).to(self.device)\n",
    "        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-7)\n",
    "\n",
    "        # convert list to tensor\n",
    "        old_states = (\n",
    "            torch.squeeze(torch.stack(self.buffer.states, dim=0))\n",
    "            .detach()\n",
    "            .to(self.device)\n",
    "        )\n",
    "        old_actions = (\n",
    "            torch.squeeze(torch.stack(self.buffer.actions, dim=0))\n",
    "            .detach()\n",
    "            .to(self.device)\n",
    "        )\n",
    "        old_logprobs = (\n",
    "            torch.squeeze(torch.stack(self.buffer.logprobs, dim=0))\n",
    "            .detach()\n",
    "            .to(self.device)\n",
    "        )\n",
    "        old_state_values = (\n",
    "            torch.squeeze(torch.stack(self.buffer.state_values, dim=0))\n",
    "            .detach()\n",
    "            .to(self.device)\n",
    "        )\n",
    "\n",
    "        # calculate advantages\n",
    "        advantages = rewards.detach() - old_state_values.detach()\n",
    "\n",
    "        # Optimize policy for K epochs\n",
    "        for _ in range(self.K_epochs):\n",
    "            # Evaluating old actions and values\n",
    "            logprobs, state_values, dist_entropy = self.policy.evaluate(\n",
    "                old_states, old_actions\n",
    "            )\n",
    "\n",
    "            # match state_values tensor dimensions with rewards tensor\n",
    "            state_values = torch.squeeze(state_values)\n",
    "\n",
    "            # Finding the ratio (pi_theta / pi_theta__old)\n",
    "            ratios = torch.exp(logprobs - old_logprobs.detach())\n",
    "\n",
    "            # Finding Surrogate Loss\n",
    "            surr1 = ratios * advantages\n",
    "            surr2 = (\n",
    "                torch.clamp(ratios, 1 - self.eps_clip, 1 + self.eps_clip) * advantages\n",
    "            )\n",
    "\n",
    "            # final loss of clipped objective PPO\n",
    "            loss = (\n",
    "                -torch.min(surr1, surr2)\n",
    "                + 0.5 * self.MseLoss(state_values, rewards)\n",
    "                - 0.01 * dist_entropy\n",
    "            )\n",
    "\n",
    "            # take gradient step\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.mean().backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "        # Copy new weights into old policy\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "\n",
    "        # clear buffer\n",
    "        self.buffer.clear()\n",
    "\n",
    "    def save(self, checkpoint_path):\n",
    "        torch.save(self.policy_old.state_dict(), checkpoint_path)\n",
    "\n",
    "    def load(self, checkpoint_path):\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=self.device)\n",
    "        self.policy_old.load_state_dict(checkpoint)\n",
    "        self.policy.load_state_dict(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "The training loop is the main part of PPO. It makes the agent better by having it interact with its environment and learn from what happens. \n",
    "\n",
    "The loop includes picking actions with the current policy, getting rewards and states, and updating the policy regularly.\n",
    "\n",
    "During training, we print updates about how many episodes we've finished, how many steps we've taken, and the average reward the agent is getting.\n",
    "\n",
    "Once training is done, the improved model can make better decisions, leading to solutions that are closer to the best possible ones.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(checkpoint_path: str, max_steps: int = 1_000_000):\n",
    "    print(\n",
    "        f\"Device set to: {torch.cuda.get_device_name(device) if torch.cuda.is_available() else 'CPU'}\"\n",
    "    )\n",
    "    print(f\"Training environments: {', '.join(env_names)}\")\n",
    "\n",
    "    max_ep_len = 50  # Max timesteps in one episode\n",
    "    print_freq = 20_000  # Frequency for printing average reward\n",
    "    save_model_freq = int(1e5)  # Model saving frequency\n",
    "    update_timestep = max_ep_len * 4  # update policy every n timesteps\n",
    "\n",
    "    envs = [\n",
    "        gym.make(name, invalid_action_response=\"raise\", verbose=False)\n",
    "        for name in env_names\n",
    "    ]\n",
    "    env = envs[0]  # Select an environment\n",
    "\n",
    "    # Initialize the PPO agent\n",
    "    ppo_agent = PPO(env.observation_space.shape[0], env.action_space.n)\n",
    "\n",
    "    # Training variables\n",
    "    time_step = 0\n",
    "    i_episode = 0\n",
    "    total_reward = 0\n",
    "\n",
    "    # Training loop\n",
    "    while time_step <= max_steps:\n",
    "        state, _ = env.reset()\n",
    "        episode_reward = 0\n",
    "\n",
    "        for t in range(1, max_ep_len + 1):\n",
    "            action = ppo_agent.select_action(state)\n",
    "            state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            ppo_agent.buffer.rewards.append(reward)\n",
    "            ppo_agent.buffer.is_terminals.append(done)\n",
    "\n",
    "            time_step += 1\n",
    "            episode_reward += reward\n",
    "\n",
    "            if time_step % update_timestep == 0:\n",
    "                ppo_agent.update()\n",
    "\n",
    "            # Print average reward\n",
    "            if time_step % print_freq == 0:\n",
    "                avg_reward = total_reward / i_episode if i_episode > 0 else 0\n",
    "                print(\n",
    "                    f\"Episode: {i_episode} \\t Timestep: {time_step} \\t Average Reward: {avg_reward:.2f}\"\n",
    "                )\n",
    "\n",
    "            # Save model\n",
    "            if time_step % save_model_freq == 0:\n",
    "                print(f\"Saving model at timestep {time_step}\")\n",
    "                ppo_agent.save(checkpoint_path)\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        total_reward += episode_reward\n",
    "        i_episode += 1\n",
    "\n",
    "    print(\"Training completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to train out model. If you want to train a fully-capable agent in this environment, you might want to train for 1 or more million steps with the default argument value, but that can take an hour or more. \n",
    "\n",
    "For our purposes a few hundred thousand steps is a good number because it should only take a few minutes and you can see the agent start to learn how to solve problems in that time.\n",
    "\n",
    "Any reward values over **0.0** almost always indicate a correct solution within the number of steps allowed by the environment. Perfect scores are generally around **~1.5** for most environments and max out at about **2.0** for others that only take a few steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device set to: NVIDIA GeForce RTX 3090\n",
      "Training environments: mathy-poly-easy-v0\n",
      "Episode: 1672 \t Timestep: 20000 \t Average Reward: -1.25\n",
      "Episode: 3752 \t Timestep: 40000 \t Average Reward: -0.54\n",
      "Episode: 6370 \t Timestep: 60000 \t Average Reward: 0.02\n",
      "Episode: 9146 \t Timestep: 80000 \t Average Reward: 0.34\n",
      "Episode: 12116 \t Timestep: 100000 \t Average Reward: 0.54\n",
      "Saving model at timestep 100000\n",
      "Episode: 15078 \t Timestep: 120000 \t Average Reward: 0.66\n",
      "Episode: 18083 \t Timestep: 140000 \t Average Reward: 0.75\n",
      "Episode: 21243 \t Timestep: 160000 \t Average Reward: 0.81\n",
      "Episode: 24630 \t Timestep: 180000 \t Average Reward: 0.88\n",
      "Episode: 27946 \t Timestep: 200000 \t Average Reward: 0.92\n",
      "Saving model at timestep 200000\n",
      "Episode: 31427 \t Timestep: 220000 \t Average Reward: 0.96\n",
      "Episode: 34634 \t Timestep: 240000 \t Average Reward: 0.99\n",
      "Episode: 38175 \t Timestep: 260000 \t Average Reward: 1.02\n",
      "Episode: 41833 \t Timestep: 280000 \t Average Reward: 1.05\n",
      "Episode: 45453 \t Timestep: 300000 \t Average Reward: 1.07\n",
      "Saving model at timestep 300000\n",
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "train(checkpoint_path, 300_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "The test function evaluates the performance of our trained PPO agent. It loads the agent's model from a saved checkpoint and runs it through multiple episodes in different environments. In each episode, the agent makes decisions based on its learned policy, and we track the rewards it earns. The main goal is to see how well the agent performs in these test scenarios, indicated by the total rewards it accumulates across episodes. This testing phase is crucial as it gives us a clear picture of the effectiveness of our training and the agent's ability to handle various challenges within the environments. The average reward per episode, calculated at the end, serves as a key metric to assess the agent's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(checkpoint_path: str):\n",
    "    envs = [\n",
    "        gym.make(name, invalid_action_response=\"raise\", verbose=True)\n",
    "        for name in env_names\n",
    "    ]\n",
    "    assert len(envs) > 0, \"No environments found\"\n",
    "    env = envs[0]\n",
    "    ppo_agent = PPO(env.observation_space.shape[0], env.action_space.n)\n",
    "    \n",
    "    print(f\"\\nloading network from : {checkpoint_path}\\n\", flush=True)\n",
    "    ppo_agent.load(checkpoint_path)\n",
    "\n",
    "    total_test_episodes = 10  # total num of testing episodes\n",
    "    test_running_reward = 0\n",
    "\n",
    "    for ep in range(1, total_test_episodes + 1):\n",
    "        env = np.random.choice(envs)\n",
    "        ep_reward = 0\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action = ppo_agent.select_action(state)\n",
    "            state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            ep_reward += reward\n",
    "\n",
    "        test_running_reward += ep_reward\n",
    "        emoji = \"âœ…\" if ep_reward >= 0.0 else \"ðŸ”´\"\n",
    "        print(f\"[{ep}]{emoji} Reward: {round(ep_reward, 2)}\")\n",
    "        ep_reward = 0\n",
    "\n",
    "    avg_test_reward = test_running_reward / total_test_episodes\n",
    "    print(f\"Average test reward: {round(avg_test_reward, 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having trained a model and written the evalutaion, we can finally see the results of our hard work. Our tiny model (< 1MB) is able to solve our polynomial simplification problems somewhat consistently. \n",
    "\n",
    "With more training the given agent config can reach near perfect accuracy on this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "006 | -- cs -- df ag -- -- | 16 | 0.0   | initial-state(-1)         | (6j^2 + 2j^2 + 2q + 5r^3)\n",
      "\n",
      "loading network from : ppo.pth\n",
      "\n",
      "005 | -- cs -- -- ag -- -- | 16 | 0.0   | initial-state(-1)         | 6y + 8m^2 + 12y + 10m^2\n",
      "007 | -- cs -- df ag -- -- | 15 | -0.01 | commutative swap(9)       | 6y + 12y \u001b[1m\u001b[32m+\u001b[0m 8m^2 + 10m^2\n",
      "008 | ca cs dm df ag -- -- | 14 | 0.01  | distributive factoring(3) | (\u001b[1m\u001b[32m6\u001b[0m \u001b[1m\u001b[32m+\u001b[0m \u001b[1m\u001b[32m12\u001b[0m) \u001b[1m\u001b[32m*\u001b[0m \u001b[1m\u001b[32my\u001b[0m + 8m^2 + 10m^2\n",
      "004 | -- cs -- df ag -- -- | 13 | 0.01  | constant arithmetic(1)    | \u001b[1m\u001b[32m18\u001b[0my + 8m^2 + 10m^2\n",
      "004 | ca cs dm -- -- -- -- | 12 | 0.01  | distributive factoring(9) | 18y + (\u001b[1m\u001b[32m8\u001b[0m \u001b[1m\u001b[32m+\u001b[0m \u001b[1m\u001b[32m10\u001b[0m) \u001b[1m\u001b[32m*\u001b[0m \u001b[1m\u001b[32mm\u001b[0m\u001b[1m\u001b[32m^\u001b[0m\u001b[1m\u001b[32m2\u001b[0m\n",
      "001 | -- cs -- -- -- -- -- | 11 | 1.4   | constant arithmetic(5)    | 18y + \u001b[1m\u001b[32m18\u001b[0mm^2\n",
      "[1]âœ… Reward: 1.42\n",
      "005 | -- cs -- -- ag -- -- | 16 | 0.0   | initial-state(-1)         | (3c^4 + 5o^3 + c^4) + 8c^2\n",
      "006 | -- cs -- df ag -- -- | 15 | -0.01 | commutative swap(11)      | 3c^4 + c^4 \u001b[1m\u001b[32m+\u001b[0m 5o^3 + 8c^2\n",
      "006 | ca cs dm -- ag -- -- | 14 | 0.01  | distributive factoring(5) | (\u001b[1m\u001b[32m3\u001b[0m \u001b[1m\u001b[32m+\u001b[0m \u001b[1m\u001b[32m1\u001b[0m) \u001b[1m\u001b[32m*\u001b[0m \u001b[1m\u001b[32mc\u001b[0m\u001b[1m\u001b[32m^\u001b[0m\u001b[1m\u001b[32m4\u001b[0m + 5o^3 + 8c^2\n",
      "003 | -- cs -- -- ag -- -- | 13 | 1.7   | constant arithmetic(1)    | \u001b[1m\u001b[32m4\u001b[0mc^4 + 5o^3 + 8c^2\n",
      "[2]âœ… Reward: 1.67\n",
      "007 | -- cs -- df ag -- -- | 16 | 0.0   | initial-state(-1)         | 7o^2 + (o^2 + g + 9g)\n",
      "007 | ca cs dm df ag -- -- | 15 | 0.01  | distributive factoring(5) | (\u001b[1m\u001b[32m7\u001b[0m \u001b[1m\u001b[32m+\u001b[0m \u001b[1m\u001b[32m1\u001b[0m) \u001b[1m\u001b[32m*\u001b[0m \u001b[1m\u001b[32mo\u001b[0m\u001b[1m\u001b[32m^\u001b[0m\u001b[1m\u001b[32m2\u001b[0m + (g + 9g)\n",
      "008 | ca cs dm -- -- -- -- | 14 | 0.01  | distributive factoring(9) | (7 + 1) * o^2 + (\u001b[1m\u001b[32m1\u001b[0m \u001b[1m\u001b[32m+\u001b[0m \u001b[1m\u001b[32m9\u001b[0m) \u001b[1m\u001b[32m*\u001b[0m \u001b[1m\u001b[32mg\u001b[0m\n",
      "005 | ca cs dm -- -- -- -- | 13 | 0.01  | constant arithmetic(1)    | \u001b[1m\u001b[32m8\u001b[0mo^2 + (1 + 9) * g\n",
      "001 | -- cs -- -- -- -- -- | 12 | 1.5   | constant arithmetic(7)    | 8o^2 + \u001b[1m\u001b[32m10\u001b[0mg\n",
      "[3]âœ… Reward: 1.53\n",
      "005 | -- cs -- -- ag -- -- | 16 | 0.0   | initial-state(-1)         | 7v + (10v^4 + v) + 7v^4\n",
      "007 | -- cs -- df ag -- -- | 15 | -0.01 | commutative swap(9)       | 7v + (v \u001b[1m\u001b[32m+\u001b[0m 10v^4) + 7v^4\n",
      "007 | ca cs dm df ag -- -- | 14 | 0.01  | distributive factoring(11) | 7v + v + (\u001b[1m\u001b[32m10\u001b[0m \u001b[1m\u001b[32m+\u001b[0m \u001b[1m\u001b[32m7\u001b[0m) \u001b[1m\u001b[32m*\u001b[0m \u001b[1m\u001b[32mv\u001b[0m\u001b[1m\u001b[32m^\u001b[0m\u001b[1m\u001b[32m4\u001b[0m\n",
      "004 | -- cs -- df ag -- -- | 13 | 0.01  | constant arithmetic(7)    | 7v + v + \u001b[1m\u001b[32m17\u001b[0mv^4\n",
      "005 | ca cs dm -- -- -- -- | 12 | 0.01  | distributive factoring(3) | (\u001b[1m\u001b[32m7\u001b[0m \u001b[1m\u001b[32m+\u001b[0m \u001b[1m\u001b[32m1\u001b[0m) \u001b[1m\u001b[32m*\u001b[0m \u001b[1m\u001b[32mv\u001b[0m + 17v^4\n",
      "001 | -- cs -- -- -- -- -- | 11 | 1.4   | constant arithmetic(1)    | \u001b[1m\u001b[32m8\u001b[0mv + 17v^4\n",
      "[4]âœ… Reward: 1.42\n",
      "006 | -- cs -- df ag -- -- | 16 | 0.0   | initial-state(-1)         | (9u^3 + 10r + 3r + 8u^3)\n",
      "007 | ca cs dm -- ag -- -- | 15 | 0.01  | distributive factoring(9) | 9u^3 + (\u001b[1m\u001b[32m10\u001b[0m \u001b[1m\u001b[32m+\u001b[0m \u001b[1m\u001b[32m3\u001b[0m) \u001b[1m\u001b[32m*\u001b[0m \u001b[1m\u001b[32mr\u001b[0m + 8u^3\n",
      "008 | ca cs dm df ag -- -- | 14 | -0.01 | commutative swap(5)       | (10 + 3) * r \u001b[1m\u001b[32m+\u001b[0m 9u^3 + 8u^3\n",
      "004 | -- cs -- df ag -- -- | 13 | 0.01  | constant arithmetic(1)    | \u001b[1m\u001b[32m13\u001b[0mr + 9u^3 + 8u^3\n",
      "004 | ca cs dm -- -- -- -- | 12 | 0.01  | distributive factoring(9) | 13r + (\u001b[1m\u001b[32m9\u001b[0m \u001b[1m\u001b[32m+\u001b[0m \u001b[1m\u001b[32m8\u001b[0m) \u001b[1m\u001b[32m*\u001b[0m \u001b[1m\u001b[32mu\u001b[0m\u001b[1m\u001b[32m^\u001b[0m\u001b[1m\u001b[32m3\u001b[0m\n",
      "001 | -- cs -- -- -- -- -- | 11 | 1.4   | constant arithmetic(5)    | 13r + \u001b[1m\u001b[32m17\u001b[0mu^3\n",
      "[5]âœ… Reward: 1.42\n",
      "003 | -- cs -- -- ag -- -- | 16 | 0.0   | initial-state(-1)         | (5p^3 + 2y + 8p^3)\n",
      "004 | -- cs -- df ag -- -- | 15 | -0.01 | commutative swap(5)       | 2y \u001b[1m\u001b[32m+\u001b[0m 5p^3 + 8p^3\n",
      "004 | ca cs dm -- -- -- -- | 14 | 0.01  | distributive factoring(9) | 2y + (\u001b[1m\u001b[32m5\u001b[0m \u001b[1m\u001b[32m+\u001b[0m \u001b[1m\u001b[32m8\u001b[0m) \u001b[1m\u001b[32m*\u001b[0m \u001b[1m\u001b[32mp\u001b[0m\u001b[1m\u001b[32m^\u001b[0m\u001b[1m\u001b[32m3\u001b[0m\n",
      "001 | -- cs -- -- -- -- -- | 13 | 1.7   | constant arithmetic(5)    | 2y + \u001b[1m\u001b[32m13\u001b[0mp^3\n",
      "[6]âœ… Reward: 1.67\n",
      "009 | -- cs -- -- ag -- -- | 20 | 0.0   | initial-state(-1)         | (4a + 4f + 5f^2 + 8a + 12z + 7f)\n",
      "009 | -- cs -- -- ag -- -- | 19 | -0.01 | commutative swap(7)       | 4a + 5f^2 \u001b[1m\u001b[32m+\u001b[0m 4f + 8a + 12z + 7f\n",
      "009 | -- cs -- -- ag -- -- | 18 | -0.01 | commutative swap(9)       | 4a + 4f \u001b[1m\u001b[32m+\u001b[0m 5f^2 + 8a + 12z + 7f\n",
      "009 | -- cs -- -- ag -- -- | 17 | -0.04 | commutative swap(7)       | 4a + 5f^2 \u001b[1m\u001b[32m+\u001b[0m 4f + 8a + 12z + 7f\n",
      "009 | -- cs -- -- ag -- -- | 16 | -0.04 | commutative swap(9)       | 4a + 4f \u001b[1m\u001b[32m+\u001b[0m 5f^2 + 8a + 12z + 7f\n",
      "009 | -- cs -- -- ag -- -- | 15 | -0.06 | commutative swap(7)       | 4a + 5f^2 \u001b[1m\u001b[32m+\u001b[0m 4f + 8a + 12z + 7f\n",
      "009 | -- cs -- -- ag -- -- | 14 | -0.01 | associative group(9)      | 4a + 5f^2 \u001b[1m\u001b[32m+\u001b[0m (4f + 8a) + 12z + 7f\n",
      "009 | -- cs -- -- ag -- -- | 13 | -0.01 | associative group(9)      | 4a + 5f^2 \u001b[1m\u001b[32m+\u001b[0m (4f + 8a + 12z) + 7f\n",
      "009 | -- cs -- -- ag -- -- | 12 | -0.01 | associative group(9)      | 4a + 5f^2 \u001b[1m\u001b[32m+\u001b[0m (4f + 8a + 12z + 7f)\n",
      "009 | -- cs -- -- ag -- -- | 11 | -0.01 | commutative swap(9)       | 4a + (4f + 8a + 12z + 7f) \u001b[1m\u001b[32m+\u001b[0m 5f^2\n",
      "009 | -- cs -- -- ag -- -- | 10 | -0.01 | commutative swap(11)      | 4a + (4f + 12z \u001b[1m\u001b[32m+\u001b[0m 8a + 7f) + 5f^2\n",
      "009 | -- cs -- -- ag -- -- | 09 | -0.04 | commutative swap(11)      | 4a + (4f + 8a \u001b[1m\u001b[32m+\u001b[0m 12z + 7f) + 5f^2\n",
      "009 | -- cs -- -- ag -- -- | 08 | -0.04 | commutative swap(11)      | 4a + (4f + 12z \u001b[1m\u001b[32m+\u001b[0m 8a + 7f) + 5f^2\n",
      "009 | -- cs -- -- ag -- -- | 07 | -0.06 | commutative swap(11)      | 4a + (4f + 8a \u001b[1m\u001b[32m+\u001b[0m 12z + 7f) + 5f^2\n",
      "009 | -- cs -- -- ag -- -- | 06 | -0.01 | commutative swap(15)      | 4a + (4f + 8a + 7f \u001b[1m\u001b[32m+\u001b[0m 12z) + 5f^2\n",
      "010 | -- cs -- df ag -- -- | 05 | -0.01 | commutative swap(11)      | 4a + (4f + 7f \u001b[1m\u001b[32m+\u001b[0m 8a + 12z) + 5f^2\n",
      "010 | -- cs -- df ag -- -- | 04 | -0.01 | commutative swap(3)       | 4f + 7f + 8a + 12z \u001b[1m\u001b[32m+\u001b[0m 4a + 5f^2\n",
      "011 | ca cs dm -- ag -- -- | 03 | 0.01  | distributive factoring(3) | (\u001b[1m\u001b[32m4\u001b[0m \u001b[1m\u001b[32m+\u001b[0m \u001b[1m\u001b[32m7\u001b[0m) \u001b[1m\u001b[32m*\u001b[0m \u001b[1m\u001b[32mf\u001b[0m + 8a + 12z + 4a + 5f^2\n",
      "007 | -- cs -- -- ag -- -- | 02 | 0.01  | constant arithmetic(1)    | \u001b[1m\u001b[32m11\u001b[0mf + 8a + 12z + 4a + 5f^2\n",
      "007 | -- cs -- -- ag -- -- | 01 | -0.01 | associative group(3)      | 11f \u001b[1m\u001b[32m+\u001b[0m (8a + 12z) + 4a + 5f^2\n",
      "007 | -- cs -- -- ag -- -- | 00 | -1.0  | associative group(3)      | 11f \u001b[1m\u001b[32m+\u001b[0m (8a + 12z + 4a) + 5f^2\n",
      "[7]ðŸ”´ Reward: -1.37\n",
      "002 | -- cs -- df -- -- -- | 08 | 0.0   | initial-state(-1)         | (12k^4 + 4k^4)\n",
      "003 | ca cs dm -- -- -- -- | 07 | 0.01  | distributive factoring(5) | (\u001b[1m\u001b[32m12\u001b[0m \u001b[1m\u001b[32m+\u001b[0m \u001b[1m\u001b[32m4\u001b[0m) \u001b[1m\u001b[32m*\u001b[0m \u001b[1m\u001b[32mk\u001b[0m\u001b[1m\u001b[32m^\u001b[0m\u001b[1m\u001b[32m4\u001b[0m\n",
      "002 | -- cs -- df -- -- -- | 06 | -0.01 | distributive multiply(3)  | \u001b[1m\u001b[32m12k^4\u001b[0m \u001b[1m\u001b[32m+\u001b[0m \u001b[1m\u001b[32m4k^4\u001b[0m\n",
      "003 | ca cs dm -- -- -- -- | 05 | -0.04 | distributive factoring(5) | (\u001b[1m\u001b[32m12\u001b[0m \u001b[1m\u001b[32m+\u001b[0m \u001b[1m\u001b[32m4\u001b[0m) \u001b[1m\u001b[32m*\u001b[0m \u001b[1m\u001b[32mk\u001b[0m\u001b[1m\u001b[32m^\u001b[0m\u001b[1m\u001b[32m4\u001b[0m\n",
      "000 | -- -- -- -- -- -- -- | 04 | 1.2   | constant arithmetic(1)    | \u001b[1m\u001b[32m16\u001b[0mk^4\n",
      "[8]âœ… Reward: 1.21\n",
      "005 | -- cs -- -- ag -- -- | 16 | 0.0   | initial-state(-1)         | 12x + (8x^2 + 1x + x^2)\n",
      "007 | -- cs -- df ag -- -- | 15 | -0.01 | commutative swap(9)       | 12x + (1x \u001b[1m\u001b[32m+\u001b[0m 8x^2 + x^2)\n",
      "008 | ca cs dm df ag -- -- | 14 | 0.01  | distributive factoring(3) | (\u001b[1m\u001b[32m12\u001b[0m \u001b[1m\u001b[32m+\u001b[0m \u001b[1m\u001b[32m1\u001b[0m) \u001b[1m\u001b[32m*\u001b[0m \u001b[1m\u001b[32mx\u001b[0m + (8x^2 + x^2)\n",
      "004 | -- cs -- df ag -- -- | 13 | 0.01  | constant arithmetic(1)    | \u001b[1m\u001b[32m13\u001b[0mx + (8x^2 + x^2)\n",
      "004 | ca cs dm -- -- -- -- | 12 | 0.01  | distributive factoring(9) | 13x + (\u001b[1m\u001b[32m8\u001b[0m \u001b[1m\u001b[32m+\u001b[0m \u001b[1m\u001b[32m1\u001b[0m) \u001b[1m\u001b[32m*\u001b[0m \u001b[1m\u001b[32mx\u001b[0m\u001b[1m\u001b[32m^\u001b[0m\u001b[1m\u001b[32m2\u001b[0m\n",
      "001 | -- cs -- -- -- -- -- | 11 | 1.4   | constant arithmetic(5)    | 13x + \u001b[1m\u001b[32m9\u001b[0mx^2\n",
      "[9]âœ… Reward: 1.42\n",
      "006 | -- cs -- df ag -- -- | 12 | 0.0   | initial-state(-1)         | c + (8c + 6c^2 + 6o)\n",
      "005 | -- cs -- -- ag -- -- | 11 | -0.01 | commutative swap(1)       | 8c + 6c^2 + 6o \u001b[1m\u001b[32m+\u001b[0m c\n",
      "005 | -- cs -- -- ag -- -- | 10 | -0.01 | commutative swap(9)       | 8c + 6o \u001b[1m\u001b[32m+\u001b[0m 6c^2 + c\n",
      "005 | -- cs -- -- ag -- -- | 09 | -0.04 | commutative swap(7)       | 8c + 6c^2 \u001b[1m\u001b[32m+\u001b[0m 6o + c\n",
      "005 | -- cs -- -- ag -- -- | 08 | -0.01 | associative group(9)      | 8c + 6c^2 \u001b[1m\u001b[32m+\u001b[0m (6o + c)\n",
      "005 | -- cs -- -- ag -- -- | 07 | -0.01 | commutative swap(9)       | 8c + (6o + c) \u001b[1m\u001b[32m+\u001b[0m 6c^2\n",
      "006 | -- cs -- df ag -- -- | 06 | -0.01 | commutative swap(7)       | 8c + (c \u001b[1m\u001b[32m+\u001b[0m 6o) + 6c^2\n",
      "007 | ca cs dm -- ag -- -- | 05 | 0.01  | distributive factoring(3) | (\u001b[1m\u001b[32m8\u001b[0m \u001b[1m\u001b[32m+\u001b[0m \u001b[1m\u001b[32m1\u001b[0m) \u001b[1m\u001b[32m*\u001b[0m \u001b[1m\u001b[32mc\u001b[0m + 6o + 6c^2\n",
      "003 | -- cs -- -- ag -- -- | 04 | 1.1   | constant arithmetic(1)    | \u001b[1m\u001b[32m9\u001b[0mc + 6o + 6c^2\n",
      "[10]âœ… Reward: 1.04\n",
      "Average test reward: 1.14\n"
     ]
    }
   ],
   "source": [
    "test(checkpoint_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
