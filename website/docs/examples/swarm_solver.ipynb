{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Swarm Planning Solver [![Open Example In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/justindujardin/mathy/blob/master/website/docs/examples/swarm_solver.ipynb)\n",
    "\n",
    "> This notebook is built using `mathy.fragile` module for swarm planning to determine which actions to take. The research and implementation come from [@Guillemdb](https://github.com/Guillemdb) and [@sergio-hcsoft](https://github.com/sergio-hcsoft). They're both amazing researchers, and we're very greatful for their research and code contributions.\n",
    "\n",
    "Sometimes training a machine learning model is inconvenient and time consuming, especially when you're working on a new problem type or set of rules. \n",
    "\n",
    "So what do we do in these cases? We use mathy's built-in swarm planning algorithm, of course!\n",
    "\n",
    "When you're developing a rule or environment, you'd often like to know how well an average agent can be expected to perform on this task, without fully-training a model each time you change the code. \n",
    "\n",
    "Let's look together at how we can use mathy.fragile to implement an agent that selects winning actions without any training, while still showing its work step-by-step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mathy in /home/forklift/mathy (0.9.6)\n",
      "Requirement already satisfied: mathy_core in /home/forklift/mathy/.env/lib/python3.8/site-packages (0.9.3)\n",
      "Requirement already satisfied: mathy_envs in /home/forklift/mathy/.env/lib/python3.8/site-packages (0.12.1)\n",
      "Requirement already satisfied: numpy in /home/forklift/mathy/.env/lib/python3.8/site-packages (from mathy) (1.24.4)\n",
      "Requirement already satisfied: colr in /home/forklift/mathy/.env/lib/python3.8/site-packages (from mathy) (0.9.1)\n",
      "Requirement already satisfied: srsly in /home/forklift/mathy/.env/lib/python3.8/site-packages (from mathy) (2.4.8)\n",
      "Requirement already satisfied: typer<0.7.0,>=0.6.1 in /home/forklift/mathy/.env/lib/python3.8/site-packages (from mathy) (0.6.1)\n",
      "Requirement already satisfied: wasabi in /home/forklift/mathy/.env/lib/python3.8/site-packages (from mathy) (1.1.2)\n",
      "Requirement already satisfied: gymnasium in /home/forklift/mathy/.env/lib/python3.8/site-packages (from mathy) (0.29.1)\n",
      "Requirement already satisfied: tqdm>=4.43.0 in /home/forklift/mathy/.env/lib/python3.8/site-packages (from mathy) (4.66.1)\n",
      "Requirement already satisfied: xxhash in /home/forklift/mathy/.env/lib/python3.8/site-packages (from mathy) (3.4.1)\n",
      "Requirement already satisfied: numba in /home/forklift/mathy/.env/lib/python3.8/site-packages (from mathy) (0.58.1)\n",
      "Requirement already satisfied: gym<0.26.0 in /home/forklift/mathy/.env/lib/python3.8/site-packages (from mathy_envs) (0.25.2)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /home/forklift/mathy/.env/lib/python3.8/site-packages (from gym<0.26.0->mathy_envs) (3.0.0)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in /home/forklift/mathy/.env/lib/python3.8/site-packages (from gym<0.26.0->mathy_envs) (0.0.8)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in /home/forklift/mathy/.env/lib/python3.8/site-packages (from gym<0.26.0->mathy_envs) (7.0.0)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/forklift/mathy/.env/lib/python3.8/site-packages (from typer<0.7.0,>=0.6.1->mathy) (8.1.7)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /home/forklift/mathy/.env/lib/python3.8/site-packages (from gymnasium->mathy) (4.9.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /home/forklift/mathy/.env/lib/python3.8/site-packages (from gymnasium->mathy) (0.0.4)\n",
      "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /home/forklift/mathy/.env/lib/python3.8/site-packages (from numba->mathy) (0.41.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.3 in /home/forklift/mathy/.env/lib/python3.8/site-packages (from srsly->mathy) (2.0.10)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/forklift/mathy/.env/lib/python3.8/site-packages (from importlib-metadata>=4.8.0->gym<0.26.0->mathy_envs) (3.17.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install mathy mathy_core mathy_envs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fractal Monte Carlo\n",
    "\n",
    "In this notebook, we'll explore the application of the Fractal Monte Carlo (FMC) algorithm using the `mathy.fragile` module and `mathy_envs`, to solve math problems step-by-step.\n",
    "\n",
    "By the time you're done with this notebook, you should have a general understanding of how FMC, through its unique path-search capabilities, interfaces with Mathy to tackle mathy's large, sparse, action spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List, Optional, Tuple, Union, cast\n",
    "\n",
    "import numpy as np\n",
    "from mathy_core import MathTypeKeysMax\n",
    "from mathy_envs import EnvRewards, MathyEnv, MathyEnvState\n",
    "from mathy_envs.gym import MathyGymEnv\n",
    "from wasabi import msg\n",
    "\n",
    "from mathy.fragile.env import DiscreteEnv\n",
    "from mathy.fragile.models import DiscreteModel\n",
    "from mathy.fragile.states import StatesEnv, StatesModel, StatesWalkers\n",
    "from mathy.fragile.swarm import Swarm\n",
    "from mathy.fragile.distributed_env import ParallelEnv\n",
    "\n",
    "\n",
    "# Use multiprocessing to speed up the swarm\n",
    "use_mp: bool = True\n",
    "# Print the step-by-step output\n",
    "verbose: bool = False\n",
    "# The number of walkers to use in the swarm\n",
    "n_walkers: int = 512\n",
    "# The number of iterations to run the swarm for\n",
    "max_iters: int = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action Selection\n",
    "\n",
    "Fragile FMC defines a \"Model\" for doing action selection for the walkers in the swarm. To aid in navigating the large sparse action space, we'll use the action mask included in mathy observations (by default) to select only valid actions at each swarm step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscreteMasked(DiscreteModel):\n",
    "    def sample(\n",
    "        self,\n",
    "        batch_size: int,\n",
    "        model_states: StatesModel,\n",
    "        env_states: StatesEnv,\n",
    "        walkers_states: StatesWalkers,\n",
    "        **kwargs\n",
    "    ) -> StatesModel:\n",
    "        if env_states is not None:\n",
    "            # Each state is a vstack([node_ids, mask]) and we only want the mask.\n",
    "            masks = env_states.observs[:, -self.n_actions :]\n",
    "            axis = 1\n",
    "            # Select a random action using the mask to filter out invalid actions\n",
    "            random_values = np.expand_dims(\n",
    "                self.random_state.rand(masks.shape[1 - axis]), axis=axis\n",
    "            )\n",
    "            actions = (masks.cumsum(axis=axis) > random_values).argmax(axis=axis)\n",
    "        else:\n",
    "            actions = self.random_state.randint(0, self.n_actions, size=batch_size)\n",
    "        return self.update_states_with_critic(\n",
    "            actions=actions,\n",
    "            model_states=model_states,\n",
    "            batch_size=batch_size,\n",
    "            **kwargs,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Planning Wrapper\n",
    "\n",
    "Because FMC uses a swarm of many workers, it's vastly more efficient if you can interact with them in batches. Fragile expects a plangym style interface that is similar to gym, but includes other things such as a `step_batch` function for stepping multiple walkers at the same time.\n",
    "\n",
    "To support batch stepping, we'll implement a wrapper environment that supports the expected plangym interface, and creates an internal mathy environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "\n",
    "class PlanningEnvironment:\n",
    "    \"\"\"Fragile Environment for solving Mathy problems.\"\"\"\n",
    "\n",
    "    problem: Optional[str]\n",
    "\n",
    "    @property\n",
    "    def unwrapped(self) -> MathyGymEnv:\n",
    "        return cast(MathyGymEnv, self._env.unwrapped)\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        name: str,\n",
    "        environment: str = \"poly\",\n",
    "        difficulty: str = \"normal\",\n",
    "        problem: Optional[str] = None,\n",
    "        max_steps: int = 64,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self._env = gym.make(\n",
    "            f\"mathy-{environment}-{difficulty}-v0\",\n",
    "            invalid_action_response=\"terminal\",\n",
    "            env_problem=problem,\n",
    "            mask_as_probabilities=True,\n",
    "            **kwargs,\n",
    "        )\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=0,\n",
    "            high=MathTypeKeysMax,\n",
    "            shape=(256, 256, 1),\n",
    "            dtype=np.uint8,\n",
    "        )\n",
    "        self.action_space = spaces.Discrete(self._env.unwrapped.action_size)\n",
    "        self.problem = problem\n",
    "        self.max_steps = max_steps\n",
    "        self._env.reset()\n",
    "\n",
    "    def get_state(self) -> np.ndarray:\n",
    "        assert self.unwrapped.state is not None, \"env required to get_state\"\n",
    "        return self.unwrapped.state.to_np(2048)\n",
    "\n",
    "    def set_state(self, state: np.ndarray):\n",
    "        assert self.unwrapped is not None, \"env required to set_state\"\n",
    "        self.unwrapped.state = MathyEnvState.from_np(state)\n",
    "        return state\n",
    "\n",
    "    def step(\n",
    "        self, action: int, state: np.ndarray = None\n",
    "    ) -> Tuple[np.ndarray, np.ndarray, Any, bool, Dict[str, object]]:\n",
    "        assert self._env is not None, \"env required to step\"\n",
    "        assert state is not None, \"only works with state stepping\"\n",
    "        self.set_state(state)\n",
    "        obs, reward, _, _, info = self._env.step(action)\n",
    "        oob = not info.get(\"valid\", False)\n",
    "        new_state = self.get_state()\n",
    "        return new_state, obs, reward, oob, info\n",
    "\n",
    "    def step_batch(\n",
    "        self,\n",
    "        actions,\n",
    "        states: Optional[Any] = None,\n",
    "        n_repeat_action: Optional[Union[int, np.ndarray]] = None,\n",
    "    ) -> tuple:\n",
    "        data = [self.step(action, state) for action, state in zip(actions, states)]\n",
    "        new_states, observs, rewards, terminals, infos = [], [], [], [], []\n",
    "        for d in data:\n",
    "            new_state, obs, _reward, end, info = d\n",
    "            new_states.append(new_state)\n",
    "            observs.append(obs)\n",
    "            rewards.append(_reward)\n",
    "            terminals.append(end)\n",
    "            infos.append(info)\n",
    "        return new_states, observs, rewards, terminals, infos\n",
    "\n",
    "    def reset(self, batch_size: int = 1):\n",
    "        assert self._env is not None, \"env required to reset\"\n",
    "        obs, info = self._env.reset()\n",
    "        return self.get_state(), obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FMC Environment\n",
    "\n",
    "To use the planning environment we need to create a formal Mathy environment that extends the discrete envrionment exposed by Fragile.\n",
    "\n",
    "There's not too much special here, we instantiate the planning environment for use in the base class, and implement the `make_transition` function to set terminal states according to mathy_envs \"done\" property.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FMCEnvironment(DiscreteEnv):\n",
    "    \"\"\"Fragile FMC Environment for solving Mathy problems.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        name: str,\n",
    "        environment: str = \"poly\",\n",
    "        difficulty: str = \"easy\",\n",
    "        problem: Optional[str] = None,\n",
    "        max_steps: int = 64,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self._env = PlanningEnvironment(\n",
    "            name=name,\n",
    "            environment=environment,\n",
    "            difficulty=difficulty,\n",
    "            problem=problem,\n",
    "            max_steps=max_steps,\n",
    "            **kwargs,\n",
    "        )\n",
    "        self._n_actions = self._env.action_space.n\n",
    "        super(DiscreteEnv, self).__init__(\n",
    "            states_shape=self._env.get_state().shape,\n",
    "            observs_shape=self._env.observation_space.shape,\n",
    "        )\n",
    "\n",
    "    def make_transitions(\n",
    "        self, states: np.ndarray, actions: np.ndarray, dt: Union[np.ndarray, int]\n",
    "    ) -> Dict[str, np.ndarray]:\n",
    "        new_states, observs, rewards, oobs, infos = self._env.step_batch(\n",
    "            actions=actions, states=states\n",
    "        )\n",
    "        terminals = [inf.get(\"done\", False) for inf in infos]\n",
    "        data = {\n",
    "            \"states\": np.array(new_states),\n",
    "            \"observs\": np.array(observs),\n",
    "            \"rewards\": np.array(rewards),\n",
    "            \"oobs\": np.array(oobs),\n",
    "            \"terminals\": np.array(terminals),\n",
    "        }\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Swarm Solver\n",
    "\n",
    "Now that we've setup a masked action selector and a batch-capable environment for planning with many walkers, we can put it all together and use the swarm to find a path to our desired solution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swarm_solve(problem: str, max_steps: int = 256, silent: bool = False) -> None:\n",
    "    def mathy_dist(x: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Calculate Euclidean distance between two arrays.\"\"\"\n",
    "        return np.linalg.norm(x - y, axis=1)\n",
    "\n",
    "    def env_callable():\n",
    "        \"\"\"Environment setup for solving the given problem.\"\"\"\n",
    "        return FMCEnvironment(\n",
    "            name=\"mathy_v0\",\n",
    "            problem=problem,\n",
    "            repeat_problem=True,\n",
    "            max_steps=max_steps,\n",
    "        )\n",
    "\n",
    "    mathy_env: MathyEnv = env_callable()._env.unwrapped.mathy\n",
    "\n",
    "    if use_mp:\n",
    "        env_callable = ParallelEnv(env_callable=env_callable)\n",
    "\n",
    "    swarm = Swarm(\n",
    "        model=lambda env: DiscreteMasked(env=env),\n",
    "        env=env_callable,\n",
    "        reward_limit=EnvRewards.WIN,\n",
    "        n_walkers=n_walkers,\n",
    "        max_epochs=max_iters,\n",
    "        reward_scale=1,\n",
    "        distance_scale=3,\n",
    "        distance_function=mathy_dist,\n",
    "        show_pbar=False,\n",
    "    )\n",
    "\n",
    "    if not silent:\n",
    "        print(f\"Solving {problem} ...\")\n",
    "    swarm.run()\n",
    "\n",
    "    if not silent:\n",
    "        if swarm.walkers.best_reward > EnvRewards.WIN:\n",
    "            last_state = MathyEnvState.from_np(swarm.walkers.states.best_state)\n",
    "            mathy_env.print_history(last_state)\n",
    "            print(f\"Solved! {problem} = {last_state.agent.problem}\")\n",
    "        else:\n",
    "            print(\"Failed to find a solution.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solving 4x + 2y + 3j^7 + 1.9x + -8y ...\n",
      "initial-state(-1)         | 4x + 2y + 3j^7 + 1.9x + -8y\n",
      "restate subtraction(17)   | 4x + 2y + 3j^7 + 1.9x \u001b[1m\u001b[32m-\u001b[0m 8y\n",
      "associative group(3)      | 4x \u001b[1m\u001b[32m+\u001b[0m (2y + 3j^7) + 1.9x - 8y\n",
      "commutative swap(3)       | 2y + 3j^7 \u001b[1m\u001b[32m+\u001b[0m 4x + 1.9x - 8y\n",
      "commutative swap(13)      | 2y + 3j^7 + 1.9x \u001b[1m\u001b[32m+\u001b[0m 4x - 8y\n",
      "distributive factoring(13) | 2y + 3j^7 + (\u001b[1m\u001b[32m1.9\u001b[0m \u001b[1m\u001b[32m+\u001b[0m \u001b[1m\u001b[32m4\u001b[0m) \u001b[1m\u001b[32m*\u001b[0m \u001b[1m\u001b[32mx\u001b[0m - 8y\n",
      "constant arithmetic(11)   | 2y + 3j^7 + \u001b[1m\u001b[32m5.9\u001b[0mx - 8y\n",
      "commutative swap(3)       | 3j^7 \u001b[1m\u001b[32m+\u001b[0m 2y + 5.9x - 8y\n",
      "restate subtraction(13)   | 3j^7 + 2y + 5.9x \u001b[1m\u001b[32m+\u001b[0m -8y\n",
      "commutative swap(13)      | 3j^7 + 2y + -8y \u001b[1m\u001b[32m+\u001b[0m 5.9x\n",
      "commutative swap(9)       | 3j^7 + -8y \u001b[1m\u001b[32m+\u001b[0m 2y + 5.9x\n",
      "distributive factoring(9) | 3j^7 + (\u001b[1m\u001b[32m-8\u001b[0m \u001b[1m\u001b[32m+\u001b[0m \u001b[1m\u001b[32m2\u001b[0m) \u001b[1m\u001b[32m*\u001b[0m \u001b[1m\u001b[32my\u001b[0m + 5.9x\n",
      "constant arithmetic(7)    | 3j^7 + \u001b[1m\u001b[32m-6\u001b[0my + 5.9x\n",
      "Solved! 4x + 2y + 3j^7 + 1.9x + -8y = 3j^7 + -6y + 5.9x\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "swarm_solve(\"4x + 2y + 3j^7 + 1.9x + -8y\")\n",
    "\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
