{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Like Terms [![Open Example In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/justindujardin/mathy/blob/master/libraries/website/examples/predicting_like_terms.ipynb)\n",
    "\n",
    "> This notebook is built using [thinc](https://thinc.ai){target=\\_blank} and [Mathy](https://mathy.ai). \n",
    "\n",
    "\n",
    "Remember in Algebra how you had to combine \"like terms\" to simplify problems? \n",
    "\n",
    "You'd see expressions like `60 + 2x^3 - 6x + x^3 + 17x` that have **5** total terms but only **4** \"like terms\". \n",
    "\n",
    "That's because `2x^3` and `x^3` are like and `-6x` and `17x` are like, while `60` doesn't have any other terms that are like it.\n",
    "\n",
    "Can we teach a model to predict that there are `4` like terms in the above expression?\n",
    "\n",
    "Let's give it a shot using [Mathy](https://mathy.ai) to generate math problems and [thinc](https://thinc.ai) to build a regression model that outputs the number of like terms in each input problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"thinc>=8.0.0a0\" mathy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sketch a Model\n",
    "\n",
    "Before we get started it can be good to have an idea of what input/output shapes we want for our model.\n",
    "\n",
    "We'll convert text math problems into lists of lists of integers, so our example (X) type can be represented using thinc's `Ints2d` type.\n",
    "\n",
    "The model will predict how many like terms there are in each sequence, so our output (Y) type can represented with the `Floats2d` type.\n",
    "\n",
    "Knowing the thinc types we want enables us to create an alias for our model, so we only have to type out the verbose generic signature once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from thinc.api import Model\n",
    "from thinc.types import Ints2d, Floats1d\n",
    "\n",
    "ModelX = Ints2d\n",
    "ModelY = Floats1d\n",
    "ModelT = Model[List[ModelX], ModelY]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode Text Inputs\n",
    "\n",
    "Mathy generates ascii-math problems and we have to encode them into integers that the model can process. \n",
    "\n",
    "To do this we'll build a vocabulary of all the possible characters we'll see, and map each input character to its index in the list.\n",
    "\n",
    "For math problems our vocabulary will include all the characters of the alphabet, numbers 0-9, and special characters like `*`, `-`, `.`, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from thinc.api import Model\n",
    "from thinc.types import Ints2d, Floats1d\n",
    "from thinc.api import Ops, get_current_ops\n",
    "\n",
    "vocab = \" .+-/^*()[]-01234567890abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "\n",
    "def encode_input(text: str) -> ModelX:\n",
    "    ops: Ops = get_current_ops()\n",
    "    indices: List[List[int]] = []\n",
    "    for c in text:\n",
    "        if c not in vocab:\n",
    "            raise ValueError(f\"'{c}' missing from vocabulary in text: {text}\")\n",
    "        indices.append([vocab.index(c)])\n",
    "    return ops.asarray2i(indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Try It**\n",
    "\n",
    "Let's try it out on some fixed data to be sure it works. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[16]\n",
      " [ 2]\n",
      " [14]]\n"
     ]
    }
   ],
   "source": [
    "outputs = encode_input(\"4+2\")\n",
    "assert outputs[0][0] == vocab.index(\"4\")\n",
    "assert outputs[1][0] == vocab.index(\"+\")\n",
    "assert outputs[2][0] == vocab.index(\"2\")\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Math Problems\n",
    "\n",
    "We'll use Mathy to generate random polynomial problems with a variable number of like terms. The generated problems will act as training data for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional, Set\n",
    "import random\n",
    "from mathy.problems import gen_simplify_multiple_terms\n",
    "\n",
    "def generate_problems(number: int, exclude: Optional[Set[str]] = None) -> List[str]:\n",
    "    if exclude is None:\n",
    "        exclude = set()\n",
    "    problems: List[str] = []\n",
    "    while len(problems) < number:\n",
    "        text, complexity = gen_simplify_multiple_terms(\n",
    "            random.randint(2, 6),\n",
    "            noise_probability=1.0,\n",
    "            noise_terms=random.randint(2, 10),\n",
    "            op=[\"+\", \"-\"],\n",
    "        )\n",
    "        assert text not in exclude, \"duplicate problem generated!\"\n",
    "        exclude.add(text)\n",
    "        problems.append(text)\n",
    "    return problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Try It**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['-7743l^3 + 3130r + -5826.8u - 4394r + 3g^4 - 1y - 1485u + 11w',\n",
       " '4d - -1525.5m^3 + 1w + 12l^4 + 3069.9w + -3559s - 1.8r + 6737.3l^4 - -2119l^4 - 3w + 1128.9a + -5600v - -2315b + 8.1u - -6832z',\n",
       " '-4868y^2 - 4548k + 9.6m + 3s - -7128d^4 + 6j^4 - 12v - 8.1t^4 + 1o^3 + 4c^4 - 2579o^3 - -4237.7q',\n",
       " '-4553l^2 - 11.7j + 10j - 8.3g - -5184m',\n",
       " '4o^2 + 2886u^3 + 5813q - 1u^3 + 4s - -6991u^3 + -9560a - -4774f + -1479z - 8.0f + 7x + 6.5h + -4397.2y + 12b',\n",
       " '1247m^4 + 3833q^2 + 1n - 11.7s - 1.3p - 618y^2 + -3821n + 2a - 2.4a - 11r - 4764w^3 + 4.5n - 2.2t + 572.9a - 3c^3',\n",
       " '1214.7f^4 + 11s - 2151k^4 - -7732q - 9q - 4l + -3697.3h + 3z + 5l - 7813.0p^3',\n",
       " '4m + 6x - 4u + 1f - 11m - 11.5d + 4.0z - 2n + 4386c^4 + 2.1q',\n",
       " '5h + 5.0h - 10.9a - 1517h + 2940o - -4178k^2 + -1748k^2',\n",
       " '1.5l - 8.1d - 7.4m^2 - 0.9a - -4580w - -8290.8k + 8.3j + 8g + -5722d - 5455s^2 + -5355r^2 + 11u^4']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_problems(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Like Terms\n",
    "\n",
    "Now that we can generate input problems, we'll need a function that can count the like terms in each one and return the value for use as a label.\n",
    "\n",
    "To accomplish this we'll use a few helpers from mathy to enumerate the terms and compare them to see if they're like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, List, Dict\n",
    "from mathy import MathExpression, ExpressionParser, get_terms, get_term_ex, TermEx\n",
    "from mathy.problems import mathy_term_string\n",
    "\n",
    "parser = ExpressionParser()\n",
    "\n",
    "def count_like_terms(input_problem: str) -> int:\n",
    "    expression: MathExpression = parser.parse(input_problem)\n",
    "    term_nodes: List[MathExpression] = get_terms(expression)\n",
    "    node_groups: Dict[str, List[MathExpression]] = {}\n",
    "    for term_node in term_nodes:\n",
    "        ex: Optional[TermEx] = get_term_ex(term_node)\n",
    "        assert ex is not None, f\"invalid expression {term_node}\"\n",
    "        key = mathy_term_string(variable=ex.variable, exponent=ex.exponent)\n",
    "        if key == \"\":\n",
    "            key = \"const\"\n",
    "        if key not in node_groups:\n",
    "            node_groups[key] = [term_node]\n",
    "        else:\n",
    "            node_groups[key].append(term_node)\n",
    "    like_terms = 0\n",
    "    for k, v in node_groups.items():\n",
    "        if len(v) <= 1:\n",
    "            continue\n",
    "        like_terms += len(v)\n",
    "    return like_terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Try It**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert count_like_terms(\"4x - 2y + q\") == 0\n",
    "assert count_like_terms(\"x + x + z\") == 2\n",
    "assert count_like_terms(\"4x + 2x - x + 7\") == 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Problem/Answer pairs\n",
    "\n",
    "Now that we can generate problems, count the number of like terms in them, and encode their text into integers, we have the pieces required to generate random problems and answers that we can train a neural network with.\n",
    "\n",
    "Let's write a function that will return a tuple of: the problem text, its encoded example form, and the output label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "from thinc.api import Ops, get_current_ops\n",
    "\n",
    "def to_example(input_problem: str) -> Tuple[str, ModelX, ModelY]:\n",
    "    ops: Ops = get_current_ops()\n",
    "    encoded_input = encode_input(input_problem)\n",
    "    like_terms = count_like_terms(input_problem)\n",
    "    return input_problem, encoded_input, ops.asarray1f([like_terms])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Try It**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x+2x [[46]\n",
      " [ 2]\n",
      " [14]\n",
      " [46]] [2.]\n"
     ]
    }
   ],
   "source": [
    "text, X, Y = to_example(\"x+2x\")\n",
    "assert text == \"x+2x\"\n",
    "assert X[0] == vocab.index(\"x\")\n",
    "assert Y[0] == 2\n",
    "print(text, X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a Model\n",
    "\n",
    "Now that we can generate X/Y values, let's define our model and verify that it can process a single input/output.\n",
    "\n",
    "For this we'll use Thinc and the `define_operators` context manager to connect the pieces together using overloaded operators for `chain` and `clone` operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from thinc.model import Model\n",
    "from thinc.api import concatenate, chain, clone, list2ragged\n",
    "from thinc.api import reduce_sum, Mish, with_array, Embed, residual\n",
    "\n",
    "def build_model(n_hidden: int, dropout: float = 0.1) -> ModelT:\n",
    "    with Model.define_operators({\">>\": chain, \"|\": concatenate, \"**\": clone}):\n",
    "        model = (\n",
    "            # Iterate over each element in the batch\n",
    "            with_array(\n",
    "                # Embed the vocab indices\n",
    "                Embed(n_hidden, len(vocab), column=0)\n",
    "                # Activate each batch of embedding sequences separately first\n",
    "                >> Mish(n_hidden, dropout=dropout)\n",
    "            )\n",
    "            # Convert to ragged so we can use the reduction layers\n",
    "            >> list2ragged()\n",
    "            # Sum the features for each batch input\n",
    "            >> reduce_sum()\n",
    "            # Process with a small resnet\n",
    "            >> residual(Mish(n_hidden, normalize=True)) ** 4\n",
    "            # Convert (batch_size, n_hidden) to (batch_size, 1)\n",
    "            >> Mish(1)\n",
    "        )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Try It**\n",
    "\n",
    "Let's pass an example through the model to make sure we have all the sizes right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1)\n"
     ]
    }
   ],
   "source": [
    "text, X, Y = to_example(\"14x + 2y - 3x + 7x\")\n",
    "m = build_model(12)\n",
    "m.initialize([X], m.ops.asarray(Y, dtype=\"f\"))\n",
    "mY = m.predict([X])\n",
    "print(mY.shape)\n",
    "assert mY.shape == (1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Training Datasets\n",
    "\n",
    "Now that we can generate examples and we have a model that can process them, let's generate random unique training and evaluation datasets.\n",
    "\n",
    "For this we'll write another helper function that can generate (n) training examples and respects an exclude list to avoid letting examples from the training/test sets overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Optional, Set, List\n",
    "\n",
    "DatasetTuple = Tuple[List[str], List[ModelX], List[ModelY]]\n",
    "\n",
    "def generate_dataset(\n",
    "    size: int,\n",
    "    exclude: Optional[Set[str]] = None,\n",
    ") -> DatasetTuple:\n",
    "    ops: Ops = get_current_ops()\n",
    "    texts: List[str] = generate_problems(size, exclude=exclude)\n",
    "    examples: List[ModelX] = []\n",
    "    labels: List[ModelY] = []\n",
    "    for i, text in enumerate(texts):\n",
    "        text, x, y = to_example(text)\n",
    "        examples.append(x)\n",
    "        labels.append(y)\n",
    "\n",
    "    return texts, examples, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Try It**\n",
    "\n",
    "Generate a small dataset to be sure everything is working as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts, x, y = generate_dataset(10)\n",
    "assert len(texts) == 10\n",
    "assert len(x) == 10\n",
    "assert len(y) == 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model Performance\n",
    "\n",
    "We're almost ready to train our model, we just need to write a function that will check a given trained model against a given dataset and return a 0-1 score of how accurate it was.\n",
    "\n",
    "We'll use this function to print the score as training progresses and print final test predictions at the end of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from wasabi import msg\n",
    "\n",
    "def evaluate_model(\n",
    "    model: ModelT,\n",
    "    *,\n",
    "    print_problems: bool = False,\n",
    "    texts: List[str],\n",
    "    X: List[ModelX],\n",
    "    Y: List[ModelY],\n",
    "):\n",
    "    Yeval = model.predict(X)\n",
    "    correct_count = 0\n",
    "    print_n = 12\n",
    "    if print_problems:\n",
    "        msg.divider(f\"eval samples max({print_n})\")\n",
    "    for text, y_answer, y_guess in zip(texts, Y, Yeval):\n",
    "        y_guess = round(float(y_guess))\n",
    "        correct = y_guess == int(y_answer)\n",
    "        print_fn = msg.fail\n",
    "        if correct:\n",
    "            correct_count += 1\n",
    "            print_fn = msg.good\n",
    "        if print_problems and print_n > 0:\n",
    "            print_n -= 1\n",
    "            print_fn(f\"Answer[{int(y_answer[0])}] Guess[{y_guess}] Text: {text}\")\n",
    "    if print_problems:\n",
    "        print(f\"Model predicted {correct_count} out of {len(X)} correctly.\")\n",
    "    return correct_count / len(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Try It**\n",
    "\n",
    "Let's try it out with an untrained model and expect to see a really sad score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts, X, Y = generate_dataset(128)\n",
    "m = build_model(12)\n",
    "m.initialize(X, m.ops.asarray(Y, dtype=\"f\"))\n",
    "# Assume the model should do so poorly as to round down to 0\n",
    "assert round(evaluate_model(m, texts=texts, X=X, Y=Y)) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Evaluate a Model\n",
    "\n",
    "The final helper function we need is one to train and evaluate a model given two input datasets. \n",
    "\n",
    "This function does a few things:\n",
    "\n",
    " 1. Create an Adam optimizer we can use for minimizing the model's prediction error.\n",
    " 2. Loop over the given training dataset (epoch) number of times.\n",
    " 3. For each epoch, make batches of (batch_size) examples. For each batch(X), predict the number of like terms (Yh) and subtract the known answers (Y) to get the prediction error. Update the model using the optimizer with the calculated error.\n",
    " 5. After each epoch, check the model performance against the evaluation dataset.\n",
    " 6. Save the model weights for the best score out of all the training epochs.\n",
    " 7. After all training is done, restore the best model and print results from the evaluation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from thinc.api import Adam\n",
    "from wasabi import msg\n",
    "import numpy\n",
    "\n",
    "def train_and_evaluate(\n",
    "    model: ModelT,\n",
    "    train_tuple: DatasetTuple,\n",
    "    eval_tuple: DatasetTuple,\n",
    "    *,\n",
    "    lr: float = 3e-3,\n",
    "    batch_size: int = 64,\n",
    "    epochs: int = 48,\n",
    ") -> float:\n",
    "    (train_texts, train_X, train_y) = train_tuple\n",
    "    (eval_texts, eval_X, eval_y) = eval_tuple\n",
    "    msg.divider(\"Train and Evaluate Model\")\n",
    "    msg.info(f\"Batch size = {batch_size}\\tEpochs = {epochs}\\tLearning Rate = {lr}\")\n",
    "\n",
    "    optimizer = Adam(lr)\n",
    "    best_score: float = 0.0\n",
    "    best_model: Optional[bytes] = None\n",
    "    for n in range(epochs):\n",
    "        loss = 0.0\n",
    "        batches = model.ops.multibatch(batch_size, train_X, train_y, shuffle=True)\n",
    "        for X, Y in batches:\n",
    "            Y = model.ops.asarray(Y, dtype=\"float32\")\n",
    "            Yh, backprop = model.begin_update(X)\n",
    "            err = Yh - Y\n",
    "            backprop(err)\n",
    "            loss += (err ** 2).sum()\n",
    "            model.finish_update(optimizer)\n",
    "        score = evaluate_model(model, texts=eval_texts, X=eval_X, Y=eval_y)\n",
    "        if score > best_score:\n",
    "            best_model = model.to_bytes()\n",
    "            best_score = score\n",
    "        print(f\"{n}\\t{score:.2f}\\t{loss:.2f}\")\n",
    "\n",
    "    if best_model is not None:\n",
    "        model.from_bytes(best_model)\n",
    "    print(f\"Evaluating with best model\")\n",
    "    score = evaluate_model(\n",
    "        model, texts=eval_texts, print_problems=True, X=eval_X, Y=eval_y\n",
    "    )\n",
    "    print(f\"Final Score: {score}\")\n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll generate the dataset first, so we can iterate on the model without having to spend time generating examples for each run. This also ensures we have the same dataset across different model runs, to make it easier to compare performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2Knerating train dataset with 8192 examples...\u001b[38;5;2m✔ Train set created with 8192 examples.\u001b[0m\n",
      "\u001b[2Knerating eval dataset with 2048 examples...\u001b[38;5;2m✔ Eval set created with 2048 examples.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "train_size = 1024 * 8\n",
    "test_size = 2048\n",
    "seen_texts: Set[str] = set()\n",
    "msg.text(f\"Generating train dataset with {train_size} examples...\")\n",
    "train_dataset = generate_dataset(train_size, seen_texts)\n",
    "msg.text(f\"Train set created with {train_size} examples.\")\n",
    "msg.text(f\"Generating eval dataset with {test_size} examples...\")\n",
    "eval_dataset = generate_dataset(test_size, seen_texts)\n",
    "msg.text(f\"Eval set created with {test_size} examples.\")\n",
    "init_x = train_dataset[1][:2]\n",
    "init_y = train_dataset[2][:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can build, train, and evaluate our model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\n",
      "========================== Train and Evaluate Model ==========================\u001b[0m\n",
      "\u001b[38;5;4mℹ Batch size = 64      Epochs = 32     Learning Rate = 0.002\u001b[0m\n",
      "0\t0.23\t25283.75\n",
      "1\t0.21\t17535.96\n",
      "2\t0.23\t17329.38\n",
      "3\t0.24\t15842.63\n",
      "4\t0.22\t15396.75\n",
      "5\t0.28\t14760.94\n",
      "6\t0.24\t14000.68\n",
      "7\t0.25\t13252.41\n",
      "8\t0.26\t12263.78\n",
      "9\t0.28\t12130.36\n",
      "10\t0.28\t11368.95\n",
      "11\t0.29\t10993.20\n",
      "12\t0.28\t10709.44\n",
      "13\t0.30\t10305.06\n",
      "14\t0.33\t10134.89\n",
      "15\t0.34\t9738.52\n",
      "16\t0.33\t9579.92\n",
      "17\t0.32\t9091.71\n",
      "18\t0.34\t8950.21\n",
      "19\t0.34\t8553.15\n",
      "20\t0.34\t8320.76\n",
      "21\t0.34\t7905.48\n",
      "22\t0.38\t7880.38\n",
      "23\t0.36\t7484.32\n",
      "24\t0.37\t7348.58\n",
      "25\t0.35\t7158.16\n",
      "26\t0.36\t6754.80\n",
      "27\t0.37\t6588.11\n",
      "28\t0.38\t6534.72\n",
      "29\t0.37\t6266.50\n",
      "30\t0.40\t6176.08\n",
      "31\t0.42\t5852.44\n",
      "Evaluating with best model\n",
      "\u001b[1m\n",
      "============================ eval samples max(12) ============================\u001b[0m\n",
      "\u001b[38;5;2m✔ Answer[6] Guess[6] Text: -7268s + 9c^4 - -3346u + -4891m + 12q^4 +\n",
      "3.8a + 8h + 10x - 1n^3 - 2.2k + 10b - -8598k - 5499b + 8496k + -5230b - 2r\u001b[0m\n",
      "\u001b[38;5;1m✘ Answer[2] Guess[3] Text: 11.0t - 11t + 2202f + -581a^3 - 10u\u001b[0m\n",
      "\u001b[38;5;2m✔ Answer[2] Guess[2] Text: 4085q^3 - 6667m - 9c + 2c + 3y\u001b[0m\n",
      "\u001b[38;5;2m✔ Answer[2] Guess[2] Text: 10h^3 + 11.0f^2 + 10r - 8091t^4 + 11b -\n",
      "1114x + 1r\u001b[0m\n",
      "\u001b[38;5;1m✘ Answer[2] Guess[4] Text: 0.9k + 575t^3 + 5975x - 4147l - 4.6j + 7r +\n",
      "0.6h^3 + 10b^2 + 1.8w - 4y + 7584.2w + 1q\u001b[0m\n",
      "\u001b[38;5;1m✘ Answer[2] Guess[4] Text: 1t^4 - 11z + 10.2h - 12s + 7374m + 954s +\n",
      "9q\u001b[0m\n",
      "\u001b[38;5;1m✘ Answer[2] Guess[4] Text: 10r + 12l - 7.8q + 2.9g - 8.3f + 3868.1a +\n",
      "7870p + -182p + 8.9z\u001b[0m\n",
      "\u001b[38;5;1m✘ Answer[2] Guess[3] Text: 3653x^4 - -8734m + 6418d - 12h^3 - 1069o -\n",
      "12p - -5812b - 7b - 9.2t - 2.1q + 7a^3\u001b[0m\n",
      "\u001b[38;5;1m✘ Answer[2] Guess[4] Text: 8x - 1a^2 - 6.4o + 1.2s^3 - -266y + 12f -\n",
      "-5511p + -3956n + 10.3v^4 + 0.5j + 3.5q - 3m - 8o\u001b[0m\n",
      "\u001b[38;5;1m✘ Answer[2] Guess[4] Text: 9335k + 6c^3 + 9.8p - 3.6u^4 - 3503f - 2.1h\n",
      "- -6713g^2 - 6.7d - -1433g^2 - 1j^4 - 10.9a^2 - 4.2s^2 + 4336n^3\u001b[0m\n",
      "\u001b[38;5;2m✔ Answer[6] Guess[6] Text: 0.9v^3 - 3x + 3g + 2k - 2926w^3 + 6b +\n",
      "-5993u^3 - 10.3t^2 + 0.5s^2 + 12z - 3585d - 1239z + 0.1d + 7980z - 761d -\n",
      "-1721f\u001b[0m\n",
      "\u001b[38;5;2m✔ Answer[2] Guess[2] Text: -713o - -6348.5x^4 - -4531z + 10.9j - 4o +\n",
      "-4799q - 11u^4 + -8290a - 9.5v\u001b[0m\n",
      "Model predicted 852 out of 2048 correctly.\n",
      "Final Score: 0.416015625\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.416015625"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = build_model(64)\n",
    "model.initialize(init_x, init_y)\n",
    "train_and_evaluate(\n",
    "    model, train_dataset, eval_dataset, lr=2e-3, batch_size=64, epochs=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.8 64-bit ('.env': virtualenv)",
   "language": "python",
   "name": "python36864bitenvvirtualenvbcc3528d06af44ca802a113b53f7d700"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
