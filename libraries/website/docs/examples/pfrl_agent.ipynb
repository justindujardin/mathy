{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PFRL_Agent",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Qjw1e52TR2j"
      },
      "source": [
        "# PFRL Mathy Agent [![Open Example In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/justindujardin/mathy/blob/master/libraries/website/docs/examples/pfrl_agent.ipynb)\n",
        "\n",
        "> This notebook is built using [pfrl](https://github.com/pfnet/pfrl){target=\\_blank} and [Mathy](https://mathy.ai).\n",
        "\n",
        "Remember in Algebra how you had to combine \"like terms\" to simplify problems?\n",
        "\n",
        "You'd see expressions like `60 + 2x^3 - 6x + x^3 + 17x` that have **5** total terms but only **4** \"like terms\".\n",
        "\n",
        "That's because `2x^3` and `x^3` are like and `-6x` and `17x` are like, while `60` doesn't have any other terms that are like it.\n",
        "\n",
        "Can we teach an agent to solve these kinds of problems step-by-step?\n",
        "\n",
        "Let's give it a shot using [Mathy](https://mathy.ai) to generate math problems and [pfrl](https://github.com/pfnet/pfrl).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BOpCRtMtTOpD",
        "outputId": "e5d9b433-9596-443e-d026-aab07a3e5dee"
      },
      "source": [
        "!pip install pfrl mathy_envs[gym]"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pfrl\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/93/cc/f26326d2a422d299cc686ae387bf1127f6ea11b2c2a85dae692eda0511f6/pfrl-0.1.0-py3-none-any.whl (149kB)\n",
            "\u001b[K     |████████████████████████████████| 153kB 5.7MB/s \n",
            "\u001b[?25hCollecting mathy_envs[gym]\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/a6/e838d117069d53cabfd505d26028948e48ee8f3213cfe94d371e8e8bf2ee/mathy_envs-0.9.3-py3-none-any.whl (43kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 6.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from pfrl) (1.18.5)\n",
            "Requirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.6/dist-packages (from pfrl) (1.7.0+cu101)\n",
            "Requirement already satisfied: gym>=0.9.7 in /usr/local/lib/python3.6/dist-packages (from pfrl) (0.17.3)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from pfrl) (7.0.0)\n",
            "Collecting pydantic>=1.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0d/70/315a190f48b22e9a3918bc050af5ccd68c2d1db322c23f5f38af1313a20a/pydantic-1.7.2-cp36-cp36m-manylinux2014_x86_64.whl (9.2MB)\n",
            "\u001b[K     |████████████████████████████████| 9.2MB 26.7MB/s \n",
            "\u001b[?25hCollecting colr\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/43/a9/75bcc155e0bf57062e77974a5aea724123de3becd69fca6f8572127c09a2/Colr-0.9.1.tar.gz (116kB)\n",
            "\u001b[K     |████████████████████████████████| 122kB 50.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: wasabi in /usr/local/lib/python3.6/dist-packages (from mathy_envs[gym]) (0.8.0)\n",
            "Collecting mathy-core>=0.8.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/3a/ca15993c9eae67825845d501f357086f7cb0fb8ac0c32d8c372c040a50a3/mathy_core-0.8.2-py3-none-any.whl (69kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 7.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.3.0->pfrl) (0.16.0)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=1.3.0->pfrl) (0.7)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.3.0->pfrl) (3.7.4.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym>=0.9.7->pfrl) (1.4.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym>=0.9.7->pfrl) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym>=0.9.7->pfrl) (1.3.0)\n",
            "Building wheels for collected packages: colr\n",
            "  Building wheel for colr (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for colr: filename=Colr-0.9.1-cp36-none-any.whl size=78233 sha256=6809961a455e732e3dc690775ccdf5c8ddfcc35e208cab12b1b883bba0cf11a5\n",
            "  Stored in directory: /root/.cache/pip/wheels/76/e4/56/3db5b327cb8c9b4f877dd2841222b6496e394ea26ac20718b0\n",
            "Successfully built colr\n",
            "Installing collected packages: pfrl, pydantic, colr, mathy-core, mathy-envs\n",
            "Successfully installed colr-0.9.1 mathy-core-0.8.2 mathy-envs-0.9.3 pfrl-0.1.0 pydantic-1.7.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8k_vUloTpzM"
      },
      "source": [
        "### Verify The Environment\n",
        "\n",
        "Before we write too much code, let's verify that we know the Mathy environment works and what kind of data we'll be working with."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V26pAS18T8Jl",
        "outputId": "505f65a5-f5dc-47b0-8c0c-4089994fc003"
      },
      "source": [
        "import gym\n",
        "from mathy_envs.gym import MathyGymEnv\n",
        "\n",
        "env_name = f\"mathy-poly-easy-v0\"\n",
        "env: MathyGymEnv = gym.make(env_name)  # type:ignore\n",
        "# Set to 0 if you have a GPU\n",
        "gpu = -1\n",
        "\n",
        "print(\"observation space:\", env.observation_space)\n",
        "print(\"action space:\", env.action_space)\n",
        "\n",
        "obs = env.reset()\n",
        "print(obs.tolist())\n",
        "print(obs.min())\n",
        "print(obs.max())\n",
        "print(obs.std())"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "observation space: Box(0.0, 1.0, (1027,), float32)\n",
            "action space: Discrete(768)\n",
            "[-3.5978141425591997e+18, 3.5978141425591997e+18, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05714285746216774, 0.20000000298023224, 1.0, 0.11428571492433548, 1.0, 0.20000000298023224, 1.0, 0.05714285746216774, 0.20000000298023224, 0.11428571492433548, 1.0, 0.20000000298023224, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5714285969734192, 0.0, 0.0, 0.7142857313156128, 0.0, 0.0, 0.5714285969734192, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "-3.5978141425591997e+18\n",
            "3.5978141425591997e+18\n",
            "1.587700204238583e+17\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wREgZD6VXaBS"
      },
      "source": [
        "### Masked Action Space\n",
        "\n",
        "As you probably noticed from the previous output, Mathy environments have quite large action spaces. \n",
        "\n",
        "The size of the action space is determined by the number of user-defined rules that the environment uses and the maximum sequence length that the environment will encode an observation of. Specifically, the action space has shape (num_rules, max_seq_len) and pads empty elements with 0.\n",
        "\n",
        "In addition to being large, the action space often contains many invalid action choices. The envrionment exports a mask of valid actions as part of the observation to allow ignoring invalid actions during selection. Specifically, the last (num_rules * max_seq_len) elements of the observation are a binary (0/1) mask where 0 indicates the action is not valid in the current state.\n",
        "\n",
        "We use the valid action mask exported by the environment to provide a custom action selector that extends PFRL's `DiscreteActionValue` class to mask the Q values so the agent can only select valid actions. This makes it *much* easier for the agent to find solutions early-on in training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJ8FkrWwU6yL"
      },
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from pfrl.action_value import DiscreteActionValue\n",
        "from torch.distributions.utils import lazy_property\n",
        "\n",
        "class MaskedDiscreteActionValue(DiscreteActionValue):\n",
        "    \"\"\"Q-function output for masked discrete action space.\n",
        "\n",
        "    Args:\n",
        "        q_values (torch.Tensor):\n",
        "            Array of Q values whose shape is (batchsize, n_actions)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, *, q_values, mask, q_values_formatter=lambda x: x):\n",
        "        super().__init__(q_values=q_values, q_values_formatter=q_values_formatter)\n",
        "        assert isinstance(q_values, torch.Tensor)\n",
        "        self.mask = mask\n",
        "        assert self.q_values.shape == self.mask.shape\n",
        "\n",
        "    @lazy_property\n",
        "    def greedy_actions(self):\n",
        "        return self.masked_q.detach().argmax(axis=1).int()\n",
        "\n",
        "    @lazy_property\n",
        "    def masked_q(self):\n",
        "        # Multiply by mask and then flip sign so that any remaining values\n",
        "        # are greater than all masked values.\n",
        "        return self.q_values.mul(self.mask).abs()\n",
        "\n",
        "    @lazy_property\n",
        "    def max(self):\n",
        "        index = self.greedy_actions.long().unsqueeze(1)\n",
        "        return self.masked_q.gather(dim=1, index=index).flatten()\n",
        "\n",
        "    def evaluate_actions(self, actions):\n",
        "        index = actions.long().unsqueeze(1)\n",
        "        return self.masked_q.gather(dim=1, index=index).flatten()\n",
        "\n",
        "    def compute_advantage(self, actions):\n",
        "        return self.evaluate_actions(actions) - self.max\n",
        "\n",
        "    def compute_double_advantage(self, actions, argmax_actions):\n",
        "        return self.evaluate_actions(actions) - self.evaluate_actions(argmax_actions)\n",
        "\n",
        "    def compute_expectation(self, beta):\n",
        "        return torch.sum(F.softmax(beta * self.masked_q) * self.masked_q, dim=1)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"MaskedDiscreteActionValue greedy_actions:{} q_values:{}\".format(\n",
        "            self.greedy_actions.detach().cpu().np(),\n",
        "            self.q_values_formatter(self.masked_q.detach().cpu().np()),\n",
        "        )\n",
        "\n",
        "    @property\n",
        "    def params(self):\n",
        "        return (self.masked_q,)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return MaskedDiscreteActionValue(\n",
        "            q_values=self.q_values[i],\n",
        "            q_values_formatter=self.q_values_formatter,\n",
        "            mask=self.mask[i],\n",
        "        )"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rp3sSDPDbDE0"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYWavvPHZlSJ"
      },
      "source": [
        "# All ones for Q values given\n",
        "q_values = torch.ones((1,512))\n",
        "# Mask out all but 2 values\n",
        "mask_values = torch.zeros((1,512))\n",
        "mask_values[0][0] = 1.0\n",
        "mask_values[0][12] = 1.0\n",
        "\n",
        "head = MaskedDiscreteActionValue(q_values=q_values, mask=mask_values)\n",
        "\n",
        "# Inspecting the masked_q property reveals only the masked elements are left\n",
        "assert head.masked_q.sum() == 2.0\n",
        "assert head.masked_q[0][0] == 1.0\n",
        "assert head.masked_q[0][12] == 1.0\n",
        "\n",
        "# All actions sampled are either 0 or 12\n",
        "for i in range(100):\n",
        "  assert head.greedy_actions in [0, 12]"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPKiMgN1Vt-T"
      },
      "source": [
        "import torch\n",
        "\n",
        "class QFunction(torch.nn.Module):\n",
        "    def __init__(self, obs_size: int, n_actions: int):\n",
        "        super().__init__()\n",
        "        self.n_actions = n_actions\n",
        "        self.h_size = 128\n",
        "        self.l1 = torch.nn.Linear(obs_size, self.h_size)\n",
        "        self.l2 = torch.nn.Linear(self.h_size + obs_size, self.h_size)\n",
        "        self.l3 = torch.nn.Linear(self.h_size + obs_size, self.h_size)\n",
        "        self.l4 = torch.nn.Linear(self.h_size + obs_size, 64)\n",
        "        self.l5 = torch.nn.Linear(64, n_actions)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = x\n",
        "        out = torch.nn.functional.relu(self.l1(out))\n",
        "        out = torch.nn.functional.relu(self.l2(torch.cat([out, x], -1)))\n",
        "        out = torch.nn.functional.relu(self.l3(torch.cat([out, x], -1)))\n",
        "        out = torch.nn.functional.relu(self.l4(torch.cat([out, x], -1)))\n",
        "        out = self.l5(out)\n",
        "\n",
        "        # The action mask is the last (n_action) values in the observation\n",
        "        batch_mask = x[:, -self.n_actions :]\n",
        "        assert batch_mask.shape == out.shape, \"mask doesn't match output\"\n",
        "        return MaskedDiscreteActionValue(q_values=out, mask=batch_mask)\n"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2KHVgdqfmCb"
      },
      "source": [
        "\n",
        "def make_agent(env: MathyGymEnv, gamma=0.9):\n",
        "    def feature_extractor(observation):\n",
        "        obs = torch.Tensor(observation).float()\n",
        "        if gpu != -1:\n",
        "            obs = obs.cuda()\n",
        "        return obs\n",
        "\n",
        "    obs_size = env.observation_space.low.size\n",
        "    n_actions = env.action_space.n\n",
        "    q_func = QFunction(obs_size, n_actions)\n",
        "    optimizer = torch.optim.Adam(q_func.parameters(), eps=1e-2)\n",
        "    # Use epsilon-greedy for exploration\n",
        "    explorer = pfrl.explorers.LinearDecayEpsilonGreedy(\n",
        "        start_epsilon=0.7,\n",
        "        end_epsilon=0.05,\n",
        "        decay_steps=50000,\n",
        "        random_action_func=env.action_space.sample,\n",
        "    )\n",
        "    replay_buffer = pfrl.replay_buffers.ReplayBuffer(capacity=10 ** 6)\n",
        "\n",
        "    # Now create an agent that will interact with the environment.\n",
        "    _agent = pfrl.agents.DoubleDQN(\n",
        "        q_func,\n",
        "        optimizer,\n",
        "        replay_buffer,\n",
        "        gamma,\n",
        "        explorer,\n",
        "        replay_start_size=1000,\n",
        "        update_interval=4,\n",
        "        target_update_interval=100,\n",
        "        phi=feature_extractor,\n",
        "        gpu=gpu,\n",
        "    )\n",
        "    return _agent\n"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbM2ryKdf-jr"
      },
      "source": [
        "import logging\n",
        "import numpy\n",
        "from collections import deque\n",
        "from typing import Any\n",
        "\n",
        "class MyLogger(logging.Logger):\n",
        "    last_msg: str\n",
        "    eval_wins_window: deque\n",
        "    eval_total_rewards_window: deque\n",
        "    wins_window: deque\n",
        "    total_rewards_window: deque\n",
        "\n",
        "    def info(self, msg: Any, *args: Any, **kwargs: Any,) -> None:\n",
        "        if not hasattr(self, \"last_msg\"):\n",
        "            self.last_msg = \"\"\n",
        "        # Training\n",
        "        if msg == \"outdir:%s step:%s episode:%s R:%s\":\n",
        "            if self.last_msg != msg:\n",
        "                print(\"\")\n",
        "                self.wins_window = deque(maxlen=100)\n",
        "                self.total_rewards_window = deque(maxlen=100)\n",
        "            total_reward = args[-1]\n",
        "            episode = args[-2]\n",
        "            step = args[-3]\n",
        "            self.wins_window.append(1.0 if total_reward > 0.0 else 0.0)\n",
        "            self.total_rewards_window.append(total_reward)\n",
        "            success_rate = (numpy.sum(self.wins_window)) / 100\n",
        "            out = \"\\rTRAIN ep:{}\\tmean:{:.2f}\\tsuccess:{:.2f}\".format(\n",
        "                episode, numpy.mean(self.total_rewards_window), success_rate\n",
        "            )\n",
        "            print(out, end=\"\")\n",
        "        # Statistics\n",
        "        elif msg == \"statistics:%s\":\n",
        "            return\n",
        "        # Evaluation\n",
        "        elif msg == \"evaluation episode %s length:%s R:%s\":\n",
        "            if self.last_msg != msg:\n",
        "                print(\"\")\n",
        "                self.eval_wins_window = deque(maxlen=100)\n",
        "                self.eval_total_rewards_window = deque(maxlen=100)\n",
        "            total_reward = args[-1]\n",
        "            episode = args[0]\n",
        "            self.eval_wins_window.append(1.0 if total_reward > 0.0 else 0.0)\n",
        "            self.eval_total_rewards_window.append(total_reward)\n",
        "            mean_r = numpy.mean(self.eval_total_rewards_window)\n",
        "            success_rate = numpy.sum(self.eval_wins_window) / 100\n",
        "            out = \"\\rEVAL ep:{} \\tmean R: {:.2f} \\twin rate: {:.2f}\".format(\n",
        "                episode, mean_r, success_rate\n",
        "            )\n",
        "            print(out, end=\"\")\n",
        "        # Unknown\n",
        "        else:\n",
        "            return\n",
        "        self.last_msg = msg\n"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53wpLUENgI4s",
        "outputId": "00c6ddec-4ac9-4675-ce98-11ef9e36a3f0"
      },
      "source": [
        "import pfrl\n",
        "\n",
        "agent = make_agent(env)\n",
        "outdir = f\"training/poly_easy_ddqn\"\n",
        "print(f\"==== Saving to: {outdir}\")\n",
        "\n",
        "pfrl.experiments.train_agent_with_evaluation(\n",
        "    agent,\n",
        "    env,\n",
        "    steps=1_000_000,  # Train the agent for [n] steps\n",
        "    eval_n_steps=None,  # We evaluate for episodes, not time\n",
        "    eval_n_episodes=100,  # [n] episodes are sampled for each evaluation\n",
        "    eval_max_episode_len=256,\n",
        "    train_max_episode_len=256,  # Maximum length of each episode\n",
        "    eval_interval=1000,\n",
        "    successful_score=10.0,\n",
        "    outdir=outdir,\n",
        "    logger=MyLogger(\"mathy_pfrl\"),\n",
        "    use_tensorboard=True,\n",
        ")\n",
        "print(\"Finished.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==== Saving to: training/poly_easy_ddqn\n",
            "\n",
            "TRAIN ep:65\tmean:-0.88\tsuccess:0.10\n",
            "EVAL ep:99 \tmean R: -1.22 \twin rate: 0.00\n",
            "TRAIN ep:141\tmean:-0.67\tsuccess:0.17\n",
            "EVAL ep:99 \tmean R: -1.23 \twin rate: 0.00\n",
            "TRAIN ep:214\tmean:-0.52\tsuccess:0.21\n",
            "EVAL ep:99 \tmean R: -1.01 \twin rate: 0.08\n",
            "TRAIN ep:291\tmean:-0.65\tsuccess:0.18\n",
            "EVAL ep:99 \tmean R: -1.23 \twin rate: 0.00\n",
            "TRAIN ep:361\tmean:-1.00\tsuccess:0.07\n",
            "EVAL ep:99 \tmean R: -1.23 \twin rate: 0.00\n",
            "TRAIN ep:423\tmean:-1.02\tsuccess:0.06\n",
            "EVAL ep:99 \tmean R: -1.22 \twin rate: 0.00\n",
            "TRAIN ep:459\tmean:-0.57\tsuccess:0.10"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}