{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ou8wySx98tlU"
   },
   "source": [
    "# Like Terms Reformer [![Open Example In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/justindujardin/mathy/blob/master/libraries/website/examples/like_terms_reformer.ipynb)\n",
    "\n",
    "\n",
    "> This notebook is built using [thinc](https://thinc.ai){target=_blank}, [PyTorch](https://pytorch.org/){target=_blank}, and the excellent [reformer_pytorch](https://github.com/lucidrains/reformer-pytorch){target=_blank} library. \n",
    "\n",
    "Remember in Algebra how you had to combine \"like terms\" to simplify problems?\n",
    "\n",
    "You'd see expressions like `60 + 2x^3 - 6x + x^3 + 17x` that have **5** total terms but only **4** \"like terms\".\n",
    "\n",
    "That's because `2x^3` and `x^3` are like and `-6x` and `17x` are like, while `60` doesn't have any other terms that are like it.\n",
    "\n",
    "Can we teach a transformer to predict that there are `4` like terms in the above expression? Can we then show that the transformer has learned about like terms by inspecting its attention heads and layers?\n",
    "\n",
    "Let's give it a shot using [reformer_pytorch](https://github.com/lucidrains/reformer-pytorch) to make our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "SAVW-nGThSmG",
    "outputId": "6c09f7a6-4543-42d5-8e82-db908e9556ec"
   },
   "outputs": [],
   "source": [
    "!pip install torch reformer_pytorch matplotlib wasabi pydantic typer\n",
    "!pip install \"git+git://github.com/justindujardin/thinc.git@feature/shim_thinc_optimizer#egg=thinc\" --upgrade\n",
    "try:\n",
    "  !nvidia-smi\n",
    "except Exception:\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "asLnnhAmcsSZ"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Any, Callable, Dict, List, Tuple, Union, cast, Optional\n",
    "\n",
    "import numpy as np\n",
    "import srsly\n",
    "import torch\n",
    "import typer\n",
    "from pydantic import BaseModel\n",
    "from reformer_pytorch import Recorder, ReformerLM\n",
    "from thinc.shims.pytorch import PyTorchShim\n",
    "from thinc.api import (\n",
    "    to_numpy,\n",
    "    Adam,\n",
    "    Ops,\n",
    "    PyTorchShim,\n",
    "    PyTorchWrapper,\n",
    "    fix_random_seed,\n",
    "    get_current_ops,\n",
    "    to_categorical,\n",
    "    CategoricalCrossentropy,\n",
    "    xp2torch,\n",
    ")\n",
    "from thinc.loss import Loss\n",
    "from thinc.types import Floats1d, Floats2d, Ints1d, Ints2d\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "from thinc.api import prefer_gpu, use_pytorch_for_gpu_memory\n",
    "if prefer_gpu():\n",
    "    use_pytorch_for_gpu_memory()\n",
    "fix_random_seed(0)\n",
    "\n",
    "TensorType = Ints2d\n",
    "class PyTorchCrossEntropy(Loss):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        normalize: bool = True,\n",
    "        size_average: Optional[bool] = None,\n",
    "        ignore_index: int = 0,\n",
    "        reduce: Optional[bool] = None,\n",
    "        reduction: str = \"mean\",\n",
    "    ):\n",
    "        self.normalize = normalize\n",
    "        self.reduction = reduction\n",
    "        self.size_average = size_average\n",
    "        self.ignore_index = ignore_index\n",
    "        self.reduce = reduce\n",
    "\n",
    "    def __call__(\n",
    "        self, guesses: Floats2d, truths: Union[Ints1d, Floats2d]\n",
    "    ) -> Tuple[Floats2d, float]:\n",
    "        return self.get_grad(guesses, truths), self.get_loss(guesses, truths)\n",
    "\n",
    "    def get_torch_loss(\n",
    "        self, guesses: \"torch.Tensor\", truths: \"torch.Tensor\", is_train: bool = False,\n",
    "    ) -> \"torch.Tensor\":\n",
    "        from torch.nn.functional import cross_entropy as torch_entropy\n",
    "\n",
    "        if is_train:\n",
    "            guesses.retain_grad()\n",
    "        loss = torch_entropy(\n",
    "            guesses,\n",
    "            truths,\n",
    "            size_average=self.size_average,\n",
    "            ignore_index=self.ignore_index,\n",
    "            reduction=self.reduction,\n",
    "            reduce=self.reduce,\n",
    "        )\n",
    "        return loss.cpu()\n",
    "\n",
    "    def get_grad(self, guesses: Floats2d, truths: Union[Ints1d, Floats2d]) -> Floats2d:\n",
    "        import torch\n",
    "\n",
    "        batch_tensor: torch.Tensor = xp2torch(guesses, requires_grad=True)\n",
    "        need_transpose = len(batch_tensor.shape) == 3\n",
    "        if need_transpose:\n",
    "            batch_tensor = batch_tensor.transpose(2, 1)\n",
    "        batch_labels = xp2torch(truths).long()\n",
    "        batch_tensor.retain_grad()\n",
    "        torch_loss = self.get_torch_loss(batch_tensor, batch_labels)\n",
    "        torch_loss.backward()\n",
    "        assert batch_tensor.grad is not None\n",
    "        difference = batch_tensor.grad.data\n",
    "        if need_transpose:\n",
    "            difference = difference.transpose(2, 1)\n",
    "        if self.normalize:\n",
    "            difference = difference / batch_tensor.shape[0]\n",
    "        return to_numpy(difference.cpu().numpy())\n",
    "\n",
    "    def get_loss(\n",
    "        self, guesses: Floats2d, truths: Union[Ints1d, Floats2d], is_train: bool = False\n",
    "    ) -> float:\n",
    "        batch_tensor = xp2torch(guesses, requires_grad=is_train)\n",
    "        batch_labels = xp2torch(truths).long()\n",
    "        if len(batch_tensor.shape) == 3:\n",
    "            batch_tensor = batch_tensor.transpose(2, 1)\n",
    "        loss = self.get_torch_loss(batch_tensor, batch_labels, is_train)\n",
    "        return float(loss.cpu().numpy())\n",
    "\n",
    "\n",
    "class ReformerLMConfig(BaseModel):\n",
    "    num_tokens: int\n",
    "    max_seq_len: int = 128\n",
    "    dim: int = 512\n",
    "    depth: int = 2\n",
    "    bucket_size: int = 64\n",
    "    heads: int = 4\n",
    "    n_hashes: int = 4\n",
    "    ff_chunks: int = 0\n",
    "    lsh_dropout: float = 0.1\n",
    "\n",
    "\n",
    "class MathyReformerConfig(BaseModel):\n",
    "    folder: str\n",
    "    train_file: str = \"/dev/null\"\n",
    "    eval_file: str = \"/dev/null\"\n",
    "    eval_batch_size: int = 128\n",
    "    save_every: int = 100\n",
    "    histogram_every: int = 100\n",
    "    validate_every: int = 100\n",
    "    print_every: int = 100\n",
    "    use_cuda: bool = True\n",
    "    use_profiler: bool = False\n",
    "\n",
    "    batch_size: int = 512\n",
    "    accumulate_every: int = 4\n",
    "    learning_rate: float = 3e-4\n",
    "\n",
    "    reformer: ReformerLMConfig\n",
    "\n",
    "    @property\n",
    "    def model_name(self) -> str:\n",
    "        \"\"\"Return the path to model.thinc file for this configuration\"\"\"\n",
    "        return os.path.join(self.folder, \"model.thinc\")\n",
    "\n",
    "    @property\n",
    "    def log_dir(self) -> str:\n",
    "        \"\"\"Return the path to store tensorboard logs in\"\"\"\n",
    "        return os.path.join(self.folder, \"tensorboard\")\n",
    "\n",
    "\n",
    "DatasetTuple = Tuple[List[TensorType], List[TensorType]]\n",
    "\n",
    "\n",
    "class MathyVocab:\n",
    "    vocab: List[str]\n",
    "    pad_token: int\n",
    "    char_to_int: Dict[str, int]\n",
    "\n",
    "    def __init__(self):\n",
    "        self.vocab = [\"\"] + list(\n",
    "            \" \\t\\n=.+-/^*()[]-?01234567890abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "        )\n",
    "        self.pad_token = self.vocab.index(\"\")\n",
    "        self.char_to_int = {char: index for index, char in enumerate(self.vocab)}\n",
    "        self.vocab_len = len(self.vocab)\n",
    "\n",
    "    def decode_text(self, tokens: TensorType) -> str:\n",
    "        \"\"\"Decode an integer tensor to produce a string\"\"\"\n",
    "        output: List[str] = []\n",
    "        for token in tokens.tolist():\n",
    "            token_index = int(token)\n",
    "            assert token_index < self.vocab_len, f\"invalid token: {token_index}\"\n",
    "            output.append(self.vocab[token_index])\n",
    "        return \"\".join(output)\n",
    "\n",
    "    def encode_text(self, text: str, pad_length: int = None) -> List[int]:\n",
    "        \"\"\"Encode text into a list of indices in the vocabulary\"\"\"\n",
    "        if pad_length is not None:\n",
    "            padding = [self.pad_token] * (pad_length - len(text))\n",
    "        else:\n",
    "            padding = []\n",
    "        indices = [self.char_to_int[c] for c in text]\n",
    "        return indices + padding\n",
    "\n",
    "\n",
    "class MathyReformer:\n",
    "    epoch: int\n",
    "    config: MathyReformerConfig\n",
    "    vocab: MathyVocab\n",
    "    optimizer: Adam\n",
    "    loss_fn: PyTorchCrossEntropy\n",
    "\n",
    "    def save(self) -> None:\n",
    "        model = os.path.join(self.config.folder, \"model.thinc\")\n",
    "        self.net.to_disk(model)\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: MathyReformerConfig,\n",
    "        vocab: MathyVocab,\n",
    "        must_exist: bool = False,\n",
    "        record_attention: bool = False,\n",
    "    ):\n",
    "        self.loss_fn = PyTorchCrossEntropy(normalize=False, reduction=\"sum\")\n",
    "        model = os.path.join(config.folder, \"model.thinc\")\n",
    "        model_config = os.path.join(config.folder, \"config.json\")\n",
    "        if os.path.exists(model):\n",
    "            config = MathyReformerConfig(**srsly.read_json(model_config))\n",
    "        elif must_exist:\n",
    "            raise ValueError(f\"model not found: {model}\")\n",
    "        else:\n",
    "            Path(model_config).parent.mkdir(exist_ok=True, parents=True)\n",
    "            srsly.write_json(model_config, config.dict())\n",
    "            print(f\"wrote model config: {model_config}\")\n",
    "        self.config = config\n",
    "        self.vocab = vocab\n",
    "        self.reformer = ReformerLM(**config.reformer.dict())\n",
    "        self.net = PyTorchWrapper(self.reformer)\n",
    "        self.epoch = 0\n",
    "        if os.path.exists(model):\n",
    "            print(f\"loading model: {model}\")\n",
    "            self.net.from_disk(model)\n",
    "            # self.optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "            # self.epoch = checkpoint.get(\"epoch\", 0)\n",
    "        if record_attention:\n",
    "            self.reformer = Recorder(self.reformer)\n",
    "        self.optimizer = Adam(config.learning_rate)\n",
    "        print(f\"model epoch: {self.epoch}\")\n",
    "\n",
    "\n",
    "def main(\n",
    "    folder: str = \"training/reformer/dev_reformer\",\n",
    "    train_file=\"like_terms_prediction.train.txt\",\n",
    "    eval_file=\"like_terms_prediction.eval.txt\",\n",
    "):\n",
    "    vocab = MathyVocab()\n",
    "    config = MathyReformerConfig(\n",
    "        folder=folder,\n",
    "        train_file=train_file,\n",
    "        eval_file=eval_file,\n",
    "        reformer=ReformerLMConfig(num_tokens=vocab.vocab_len),\n",
    "    )\n",
    "    print(f\"Folder: {config.folder}\")\n",
    "    print(f\"Config: {json.dumps(config.dict(), indent=2)}\")\n",
    "    reformer: MathyReformer = MathyReformer(\n",
    "        config=config, vocab=vocab,\n",
    "    )\n",
    "    pr = None\n",
    "    if config.use_profiler:\n",
    "        import cProfile\n",
    "\n",
    "        pr = cProfile.Profile()\n",
    "        pr.enable()\n",
    "        print(f\"PROFILER: recording\")\n",
    "    train(reformer, config=config, epochs=3000)\n",
    "\n",
    "    if config.use_profiler:\n",
    "        assert pr is not None\n",
    "        profile_path = os.path.join(config.folder, \"training.profile\")\n",
    "        pr.disable()\n",
    "        pr.dump_stats(profile_path)\n",
    "        print(f\"PROFILER: saved {profile_path}\")\n",
    "\n",
    "\n",
    "def load_dataset(file_name: str, pad_length: int, model: MathyReformer) -> DatasetTuple:\n",
    "    \"\"\"Load a dataset where question/answer pairs are separated by newlines, and\n",
    "    pad the outputs to match the transformer sequence length.\"\"\"\n",
    "    with open(file_name) as f:\n",
    "        lines = f.readlines()\n",
    "    in_lines: List[TensorType] = []\n",
    "    out_lines: List[TensorType] = []\n",
    "    for i, l in tqdm(enumerate(lines), desc=f\"loading dataset: {file_name}\"):\n",
    "        encoded = np.asarray(model.vocab.encode_text(l, pad_length), dtype=\"int64\")\n",
    "        if i % 2 == 0:\n",
    "            in_lines.append(encoded)\n",
    "        else:\n",
    "            out_lines.append(encoded)\n",
    "    assert len(in_lines) == len(out_lines), \"in/out files must have 1-to-1 line mapping\"\n",
    "    in_lines = model.net.ops.asarray2i(in_lines, dtype=\"int64\")\n",
    "    out_lines = model.net.ops.asarray2i(out_lines, dtype=\"int64\")\n",
    "    return in_lines, out_lines\n",
    "\n",
    "\n",
    "def cycle(loader):\n",
    "    \"\"\"Cycle through a dataset forever\"\"\"\n",
    "    while True:\n",
    "        for data in loader:\n",
    "            yield data\n",
    "\n",
    "\n",
    "def evaluate_model(\n",
    "    model: MathyReformer, dataset: DatasetTuple\n",
    ") -> Tuple[float, float, List[str]]:\n",
    "    \"\"\"Evaluate a model on a dataset and return a tuple of the total number\n",
    "    of problems evaluated, the number answered correctly, and the total loss \"\"\"\n",
    "    with torch.no_grad():\n",
    "        ops: Ops = get_current_ops()\n",
    "        batches = ops.multibatch(\n",
    "            model.config.eval_batch_size, dataset[0], dataset[1], shuffle=True\n",
    "        )\n",
    "        model.reformer.eval()\n",
    "        losses: List[float] = []\n",
    "        correct: int = 0\n",
    "        total: int = len(dataset[0])\n",
    "        print_max = 3\n",
    "        printed = 0\n",
    "        texts = []\n",
    "        for batch_with_labels in batches:\n",
    "            batch, batch_labels = batch_with_labels\n",
    "            # Check correct/incorrect answers\n",
    "            # TODO: remove the need for this torch/ops/long conversion\n",
    "            batch = xp2torch(ops.asarray(batch, dtype=\"int64\")).long()\n",
    "            if model.config.use_cuda:\n",
    "                batch = batch.cuda()\n",
    "            prediction = model.net(batch, is_train=False)[0]\n",
    "            answer: Any\n",
    "            for X, label, answer in zip(batch, batch_labels, prediction):\n",
    "                label = xp2torch(model.net.ops.asarray2i(label))\n",
    "                expected = model.vocab.decode_text(label).replace(\"\\n\", \"\")\n",
    "                # argmax resolves the class probs to ints, and squeeze removes extra dim\n",
    "                answer = model.vocab.decode_text(answer.argmax(-1).squeeze())\n",
    "                if \"\\n\" in answer:\n",
    "                    answer = answer[0 : answer.index(\"\\n\")]\n",
    "                if printed < print_max:\n",
    "                    printed += 1\n",
    "                    question = model.vocab.decode_text(X).replace(\"\\n\", \"\")\n",
    "                    outcome = \"WRONG\" if expected != answer else \"RIGHT\"\n",
    "                    print_text = f\"{outcome} | answer: {expected} | model: {answer} | question: {question}\"\n",
    "                    texts.append(print_text)\n",
    "                if answer == expected:\n",
    "                    correct += 1\n",
    "            batch_loss = get_batch_loss(model, batch_with_labels, prediction=prediction)\n",
    "            batch_loss.squeeze()\n",
    "            losses.append(float(batch_loss.mean()))\n",
    "        ratio = correct / total\n",
    "        print(\n",
    "            f\"evaluation accuracy: {int(ratio * 100)}% | correct: ({correct}/{total})\"\n",
    "        )\n",
    "        loss = model.net.ops.asarray1f(losses).mean()\n",
    "        return loss, ratio, texts\n",
    "\n",
    "\n",
    "def train(\n",
    "    model: MathyReformer, *, config: MathyReformerConfig, epochs: int,\n",
    "):\n",
    "    summary = SummaryWriter(os.path.join(config.log_dir), flush_secs=30)\n",
    "    data_train = load_dataset(config.train_file, config.reformer.max_seq_len, model)\n",
    "    data_val = load_dataset(config.eval_file, config.reformer.max_seq_len, model)\n",
    "    ops: Ops = get_current_ops()\n",
    "    train_loader = cycle(\n",
    "        model.net.ops.multibatch(\n",
    "            model.config.batch_size, data_train[0], data_train[1], shuffle=True\n",
    "        )\n",
    "    )\n",
    "    try:\n",
    "        for i in range(epochs):\n",
    "            model.epoch += 1\n",
    "            model.reformer.train()\n",
    "\n",
    "            step_start = time.time()\n",
    "            losses: List[float] = []\n",
    "            for __ in range(config.accumulate_every):\n",
    "                batch_loss = get_batch_loss(model, next(train_loader))\n",
    "                batch_loss.squeeze()\n",
    "                losses.append(float(batch_loss.mean()))\n",
    "\n",
    "            loss = float(\n",
    "                model.net.ops.asarray1f(losses).mean() / config.accumulate_every\n",
    "            )\n",
    "            print(\".\", end=\"\", flush=True)\n",
    "            step_end = time.time()\n",
    "            summary.add_scalar(\"metrics/epoch_time\", step_end - step_start, model.epoch)\n",
    "            summary.add_scalar(\"loss/train\", float(loss), model.epoch)\n",
    "            torch.nn.utils.clip_grad_norm_(model.reformer.parameters(), 0.5)\n",
    "            # model.optimizer.step()\n",
    "            # run before zero-grad\n",
    "            if i % config.histogram_every == 0:\n",
    "                for tag, value in model.reformer.named_parameters():\n",
    "                    tag = tag.replace(\".\", \"/\")\n",
    "                    summary.add_histogram(\n",
    "                        tag + \"/data\", value.data.cpu().numpy(), model.epoch\n",
    "                    )\n",
    "                    summary.add_histogram(\n",
    "                        tag + \"/gradient\", value.grad.data.cpu().numpy(), model.epoch\n",
    "                    )\n",
    "            model.net.finish_update(model.optimizer)\n",
    "            model.optimizer.step_schedules()\n",
    "            # model.optimizer.zero_grad()\n",
    "\n",
    "            if i % config.print_every == 0:\n",
    "                print(f\"step: {model.epoch} | loss: {loss}\")\n",
    "\n",
    "            if i % config.save_every == 0:\n",
    "                if i > 0:\n",
    "                    model.save()\n",
    "            if i % config.validate_every == 0:\n",
    "                eval_loss, eval_win_pct, texts = evaluate_model(model, data_val)\n",
    "                print(f\"loss_train: {loss} | loss_eval: {eval_loss}\")\n",
    "                summary.add_scalar(\"loss/eval\", float(eval_loss), model.epoch)\n",
    "                summary.add_scalar(\n",
    "                    \"metrics/eval_correct_pct\", eval_win_pct, model.epoch\n",
    "                )\n",
    "                for i, text in enumerate(texts):\n",
    "                    summary.add_text(f\"metrics/eval_sample_{i}\", text, model.epoch)\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "\n",
    "    summary.close()\n",
    "    model.save()\n",
    "\n",
    "\n",
    "def get_batch_loss(\n",
    "    model: MathyReformer,\n",
    "    data: Tuple[TensorType, Tuple[TensorType, TensorType]],\n",
    "    prediction: TensorType = None,\n",
    ") -> Floats1d:\n",
    "    x, label = data\n",
    "\n",
    "    # Allow passing a prediction to avoid duplicate model calls\n",
    "    backprop = None\n",
    "    if prediction is None:\n",
    "        prediction, backprop = model.net.begin_update(xp2torch(x.astype(\"int64\")))\n",
    "\n",
    "    # TODO: remove this reshape when/if to_categorical does it automatically\n",
    "    # label = to_categorical(label, n_classes=prediction.shape[-1]).reshape(\n",
    "    #     prediction.shape\n",
    "    # )\n",
    "    loss = model.loss_fn.get_loss(prediction, label)\n",
    "    if backprop is not None:\n",
    "        d_loss = xp2torch(model.loss_fn.get_grad(prediction, label))\n",
    "        if model.config.use_cuda:\n",
    "            d_loss = d_loss.cuda()\n",
    "        backprop(d_loss)\n",
    "    return model.net.ops.asarray1f(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "38WiKujNUhEa",
    "outputId": "01a66a4f-0204-4a77-ded0-d2569b3ecc3a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder: training/reformer/dev_reformer\n",
      "Config: {\n",
      "  \"folder\": \"training/reformer/dev_reformer\",\n",
      "  \"train_file\": \"like_terms_prediction.train.txt\",\n",
      "  \"eval_file\": \"like_terms_prediction.eval.txt\",\n",
      "  \"eval_batch_size\": 128,\n",
      "  \"save_every\": 100,\n",
      "  \"histogram_every\": 100,\n",
      "  \"validate_every\": 100,\n",
      "  \"print_every\": 100,\n",
      "  \"use_cuda\": true,\n",
      "  \"use_profiler\": false,\n",
      "  \"batch_size\": 512,\n",
      "  \"accumulate_every\": 4,\n",
      "  \"learning_rate\": 0.0003,\n",
      "  \"reformer\": {\n",
      "    \"num_tokens\": 80,\n",
      "    \"max_seq_len\": 128,\n",
      "    \"dim\": 512,\n",
      "    \"depth\": 2,\n",
      "    \"bucket_size\": 64,\n",
      "    \"heads\": 4,\n",
      "    \"n_hashes\": 4,\n",
      "    \"ff_chunks\": 0,\n",
      "    \"lsh_dropout\": 0.1\n",
      "  }\n",
      "}\n",
      "wrote model config: training/reformer/dev_reformer/config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading dataset: like_terms_prediction.train.txt: 6670it [00:00, 66698.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading dataset: like_terms_prediction.train.txt: 400000it [00:06, 66001.07it/s]\n",
      "loading dataset: like_terms_prediction.eval.txt: 2000it [00:00, 48498.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".step: 1 | loss: 1219.503662109375\n",
      "evaluation accuracy: 0% | correct: (5/1000)\n",
      "loss_train: 1219.503662109375 | loss_eval: 797.3507\n",
      "....................................................................................................step: 101 | loss: 163.42306518554688\n",
      "evaluation accuracy: 48% | correct: (483/1000)\n",
      "loss_train: 163.42306518554688 | loss_eval: 155.58597\n",
      "....................................................................................................step: 201 | loss: 149.3126678466797\n",
      "evaluation accuracy: 51% | correct: (519/1000)\n",
      "loss_train: 149.3126678466797 | loss_eval: 144.91866\n",
      "....................................................................................................step: 301 | loss: 138.98428344726562\n",
      "evaluation accuracy: 57% | correct: (571/1000)\n",
      "loss_train: 138.98428344726562 | loss_eval: 133.37936\n",
      "....................................................................................................step: 401 | loss: 135.4252471923828\n",
      "evaluation accuracy: 56% | correct: (566/1000)\n",
      "loss_train: 135.4252471923828 | loss_eval: 132.99727\n",
      "....................................................................................................step: 501 | loss: 133.71649169921875\n",
      "evaluation accuracy: 57% | correct: (574/1000)\n",
      "loss_train: 133.71649169921875 | loss_eval: 132.64777\n",
      "....................................................................................................step: 601 | loss: 129.60745239257812\n",
      "evaluation accuracy: 61% | correct: (613/1000)\n",
      "loss_train: 129.60745239257812 | loss_eval: 126.70494\n",
      "....................................................................................................step: 701 | loss: 122.04228210449219\n",
      "evaluation accuracy: 62% | correct: (626/1000)\n",
      "loss_train: 122.04228210449219 | loss_eval: 119.77521\n",
      "....................................................................................................step: 801 | loss: 114.01881408691406\n",
      "evaluation accuracy: 64% | correct: (648/1000)\n",
      "loss_train: 114.01881408691406 | loss_eval: 113.77133\n",
      "....................................................................................................step: 901 | loss: 108.57820129394531\n",
      "evaluation accuracy: 65% | correct: (655/1000)\n",
      "loss_train: 108.57820129394531 | loss_eval: 110.04442\n",
      "....................................................................................................step: 1001 | loss: 108.431640625\n",
      "evaluation accuracy: 65% | correct: (652/1000)\n",
      "loss_train: 108.431640625 | loss_eval: 109.68504\n",
      "....................................................................................................step: 1101 | loss: 114.97943115234375\n",
      "evaluation accuracy: 64% | correct: (648/1000)\n",
      "loss_train: 114.97943115234375 | loss_eval: 111.26749\n",
      "....................................................................................................step: 1201 | loss: 107.43222045898438\n",
      "evaluation accuracy: 65% | correct: (656/1000)\n",
      "loss_train: 107.43222045898438 | loss_eval: 107.309746\n",
      "....................................................................................................step: 1301 | loss: 108.95185852050781\n",
      "evaluation accuracy: 65% | correct: (654/1000)\n",
      "loss_train: 108.95185852050781 | loss_eval: 107.30882\n",
      "....................................................................................................step: 1401 | loss: 112.26199340820312\n",
      "evaluation accuracy: 65% | correct: (651/1000)\n",
      "loss_train: 112.26199340820312 | loss_eval: 106.98419\n",
      "....................................................................................................step: 1501 | loss: 106.8240737915039\n",
      "evaluation accuracy: 65% | correct: (653/1000)\n",
      "loss_train: 106.8240737915039 | loss_eval: 106.37789\n",
      "....................................................................................................step: 1601 | loss: 105.31365966796875\n",
      "evaluation accuracy: 66% | correct: (665/1000)\n",
      "loss_train: 105.31365966796875 | loss_eval: 105.86846\n",
      "....................................................................................................step: 1701 | loss: 108.36333465576172\n",
      "evaluation accuracy: 66% | correct: (661/1000)\n",
      "loss_train: 108.36333465576172 | loss_eval: 106.76781\n",
      "....................................................................................................step: 1801 | loss: 104.20068359375\n",
      "evaluation accuracy: 65% | correct: (654/1000)\n",
      "loss_train: 104.20068359375 | loss_eval: 105.77577\n",
      "....................................................................................................step: 1901 | loss: 106.36077880859375\n",
      "evaluation accuracy: 66% | correct: (661/1000)\n",
      "loss_train: 106.36077880859375 | loss_eval: 106.480286\n",
      "....................................................................................................step: 2001 | loss: 105.81416320800781\n",
      "evaluation accuracy: 66% | correct: (660/1000)\n",
      "loss_train: 105.81416320800781 | loss_eval: 105.802826\n",
      "....................................................................................................step: 2101 | loss: 107.98883056640625\n",
      "evaluation accuracy: 65% | correct: (652/1000)\n",
      "loss_train: 107.98883056640625 | loss_eval: 108.55791\n",
      "....................................................................................................step: 2201 | loss: 108.25115966796875\n",
      "evaluation accuracy: 65% | correct: (656/1000)\n",
      "loss_train: 108.25115966796875 | loss_eval: 110.0211\n",
      "....................................................................................................step: 2301 | loss: 103.73287963867188\n",
      "evaluation accuracy: 66% | correct: (661/1000)\n",
      "loss_train: 103.73287963867188 | loss_eval: 105.552765\n",
      "....................................................................................................step: 2401 | loss: 94.80268096923828\n",
      "evaluation accuracy: 70% | correct: (704/1000)\n",
      "loss_train: 94.80268096923828 | loss_eval: 91.30623\n",
      "....................................................................................................step: 2501 | loss: 63.13200378417969\n",
      "evaluation accuracy: 80% | correct: (809/1000)\n",
      "loss_train: 63.13200378417969 | loss_eval: 58.11401\n",
      "....................................................................................................step: 2601 | loss: 22.000757217407227\n",
      "evaluation accuracy: 94% | correct: (941/1000)\n",
      "loss_train: 22.000757217407227 | loss_eval: 21.18526\n",
      "....................................................................................................step: 2701 | loss: 9.245500564575195\n",
      "evaluation accuracy: 95% | correct: (950/1000)\n",
      "loss_train: 9.245500564575195 | loss_eval: 14.451437\n",
      "....................................................................................................step: 2801 | loss: 9.164619445800781\n",
      "evaluation accuracy: 99% | correct: (991/1000)\n",
      "loss_train: 9.164619445800781 | loss_eval: 3.9043887\n",
      "....................................................................................................step: 2901 | loss: 5.663301944732666\n",
      "evaluation accuracy: 98% | correct: (989/1000)\n",
      "loss_train: 5.663301944732666 | loss_eval: 4.055765\n",
      "..................................................................................................."
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Like Terms Prediction - Reformer",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
