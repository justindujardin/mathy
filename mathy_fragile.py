import copy
import sys
import time
import traceback
import gym
import numpy as np
import srsly
from gym import spaces

from fragile.core.dt_sampler import GaussianDt
from fragile.core.env import DiscreteEnv
from fragile.core.models import DiscreteUniform
from fragile.core.states import StatesEnv, StatesModel
from fragile.core.swarm import Swarm
from fragile.core.tree import HistoryTree
from fragile.core.utils import StateDict, relativize
from fragile.core.walkers import Walkers
from mathy import MathTypeKeysMax, MathyEnvState
from mathy.envs.gym import MathyGymEnv
from plangym import ParallelEnvironment
from plangym.env import Environment


class MathyWalker(Walkers):
    """
    This Walkers incorporate an additional stopping mechanism for the walkers \
    that allows to set a maximum score, and finish if the a given game has been \
    completely cleared.
    """

    def __init__(self, max_reward: int = None, *args, **kwargs):
        """
        Initialize a :class:`MathyWalker`.

        Args:
            max_reward: If the accumulated reward of the :class:`MathyWalker` \
                        reaches this values the algorithm will stop.
            *args: :class:`Walkers` parameters.
            **kwargs: :class:`Walkers` parameters.

        """
        super(MathyWalker, self).__init__(*args, **kwargs)
        self.max_reward = max_reward if max_reward is not None else 1e6

    def calculate_end_condition(self) -> bool:
        """
        Process data from the current state to decide if the iteration process 
        should stop. It not only keeps track of the maximum number of iterations 
        and the death condition, but also keeps track if the game has been played 
        until it finished.

        Returns:
            Boolean indicating if the iteration process should be finished. ``True``
            means it should be stopped, and ``False`` means it should continue.

        """
        end = super(MathyWalker, self).calculate_end_condition()
        return bool(
            self.env_states.game_ends.all()
            and end
            or self.states.cum_rewards.sum() > self.max_reward
        )


class FragileMathyEnv(DiscreteEnv):
    """FragileMathyEnv is an interface between the `plangym.Environment` and a
    Mathy environment."""

    STATE_CLASS = StatesEnv

    def get_params_dict(self) -> StateDict:
        """Return a dictionary containing the param_dict to build an instance \
        of States that can handle all the data generated by the environment.
        """
        super_params = super(FragileMathyEnv, self).get_params_dict()
        params = {"game_ends": {"dtype": np.bool_}}
        params.update(super_params)
        return params

    def step(self, model_states: StatesModel, env_states: StatesEnv) -> StatesEnv:
        """
        Set the environment to the target states by applying the specified \
        actions an arbitrary number of time steps.

        Args:
            model_states: States representing the data to be used to act on the environment.
            env_states: States representing the data to be set in the environment.

        Returns:
            States containing the information that describes the new state of the Environment.

        """
        actions = model_states.actions.astype(np.int32)
        n_repeat_actions = model_states.dt if hasattr(model_states, "dt") else 1
        new_states, observs, rewards, ends, infos = self._env.step_batch(
            actions=actions, states=env_states.states, n_repeat_action=n_repeat_actions
        )
        game_ends = [inf["done"] for inf in infos]
        new_state = self.states_from_data(
            states=new_states,
            observs=observs,
            rewards=rewards,
            ends=ends,
            batch_size=len(actions),
            game_ends=game_ends,
        )
        return new_state

    def reset(self, batch_size: int = 1, **kwargs) -> StatesEnv:
        """
        Reset the environment to the start of a new episode and returns a new \
        :class:`StatesEnv` instance describing the state of the :class:`AtariEnvironment`.

        Args:
            batch_size: Number of walkers of the returned state.
            **kwargs: Ignored. This environment resets without using any external data.

        Returns:
            :class:`StatesEnv` instance describing the state of the Environment. \
            The first dimension of the data tensors (number of walkers) will be \
            equal to batch_size.

        """
        state, obs = self._env.reset()
        states = np.array([copy.deepcopy(state) for _ in range(batch_size)])
        observs = np.array([copy.deepcopy(obs) for _ in range(batch_size)])
        rewards = np.zeros(batch_size, dtype=np.float32)
        ends = np.zeros(batch_size, dtype=np.bool_)
        game_ends = np.zeros(batch_size, dtype=np.bool_)
        new_states = self.states_from_data(
            states=states,
            observs=observs,
            rewards=rewards,
            ends=ends,
            batch_size=batch_size,
            game_ends=game_ends,
        )
        return new_states


class FragileEnvironment(Environment):
    """Fragile Environment for solving Mathy problems."""

    def __init__(
        self,
        name: str,
        n_repeat_action: int = 1,
        height: float = 32,
        width: float = 32,
        wrappers=None,
        **kwargs,
    ):
        self._env_kwargs = kwargs
        self.height = height
        self.width = width
        super(FragileEnvironment, self).__init__(
            name=name, n_repeat_action=n_repeat_action
        )
        environment = "poly"
        difficulty = "easy"
        self._env: MathyGymEnv = gym.make(f"mathy-{environment}-{difficulty}-v0")
        if height is not None and width is not None:
            self.observation_space = spaces.Box(
                low=0,
                high=MathTypeKeysMax,
                shape=(self.height, self.width, 1),
                dtype=np.uint8,
            )
        self.wrappers = wrappers
        self.init_env()

    def init_env(self):
        env = self._env
        env.reset()
        if self.wrappers is not None:
            for wrap in self.wrappers:
                env = wrap(env)
        self.action_space = spaces.Discrete(self._env.action_size)
        self.observation_space = (
            self._env.observation_space
            if self.observation_space is None
            else self.observation_space
        )

    def __getattr__(self, item):
        return getattr(self._env, item)

    def get_state(self) -> np.ndarray:
        assert self._env is not None, "env required to get_state"
        return self._env.state.to_np()

    def set_state(self, state: np.ndarray):
        assert self._env is not None, "env required to set_state"
        self._env.state = MathyEnvState.from_np(state)
        return state

    def step(
        self, action: np.ndarray, state: np.ndarray = None, n_repeat_action: int = None
    ) -> tuple:
        assert self._env is not None, "env required to step"
        if state is not None:
            self.set_state(state)
        obs, reward, _, info = self._env.step(action)
        terminal = info.get("done", False)
        if state is not None:
            new_state = self.get_state()
            return new_state, obs, reward, terminal, info
        return obs, reward, terminal, info

    def step_batch(
        self, actions, states=None, n_repeat_action: [int, np.ndarray] = None
    ) -> tuple:
        """

        :param actions:
        :param states:
        :param n_repeat_action:
        :return:
        """
        n_repeat_action = (
            n_repeat_action if n_repeat_action is not None else self.n_repeat_action
        )
        n_repeat_action = (
            n_repeat_action.astype("i")
            if isinstance(n_repeat_action, np.ndarray)
            else np.ones(len(states)) * n_repeat_action
        )
        data = [
            self.step(action, state, n_repeat_action=dt)
            for action, state, dt in zip(actions, states, n_repeat_action)
        ]
        new_states, observs, rewards, terminals, infos = [], [], [], [], []
        for d in data:
            if states is None:
                obs, _reward, end, info = d
            else:
                new_state, obs, _reward, end, info = d
                new_states.append(new_state)
            observs.append(obs)
            rewards.append(_reward)
            terminals.append(end)
            infos.append(info)
        if states is None:
            return observs, rewards, terminals, infos
        else:
            return new_states, observs, rewards, terminals, infos

    def reset(self, return_state: bool = True):
        assert self._env is not None, "env required to reset"
        obs = self._env.reset().to_np()
        # obs = (
        #     resize_frame(obs, self.height, self.width)
        #     if self.width is not None and self.height is not None
        #     else obs
        # )
        if not return_state:
            return obs
        else:
            return self.get_state(), obs


#
# Ms Pacman
#

use_mp = False
if use_mp:
    env = ParallelEnvironment(
        env_class=FragileEnvironment,
        name="arc_v0",
        clone_seeds=True,
        autoreset=True,
        blocking=False,
    )
else:
    env = FragileEnvironment(name="arc_v0")


def arc_dist(x: np.ndarray, y: np.ndarray) -> np.ndarray:
    min_len = min([len(item[0]) for item in x] + [len(item[0]) for item in y])
    x = np.array([item[0][:min_len] for item in x])
    y = np.array([item[0][:min_len] for item in y])
    return np.linalg.norm(x - y, axis=1)


dt = GaussianDt(min_dt=6, max_dt=1000, loc_dt=4, scale_dt=2)
prune_tree = True
n_walkers = 32  # A bigger number will increase the quality of the trajectories sampled.
max_iters = 100  # Increase to sample longer games.
reward_scale = 2  # Rewards are more important than diversity.
distance_scale = 1
minimize = False  # We want to get the maximum score possible.
swarm = Swarm(
    model=lambda env: DiscreteUniform(env=env, critic=dt),
    walkers=MathyWalker,
    env=lambda: FragileMathyEnv(env=env),
    tree=HistoryTree,
    n_walkers=n_walkers,
    max_iters=max_iters,
    prune_tree=prune_tree,
    reward_scale=reward_scale,
    distance_scale=distance_scale,
    distance_function=arc_dist,
    minimize=minimize,
)
_ = swarm.run_swarm(print_every=50)
best_ix = swarm.walkers.states.cum_rewards.argmax()
best_id = swarm.walkers.states.id_walkers[best_ix]
path = swarm.tree.get_branch(best_id, from_hash=True)


for s, a in zip(path[0][1:], path[1]):
    env.step(state=s, action=a)
    env.render()
    time.sleep(0.05)

print("Done!")
